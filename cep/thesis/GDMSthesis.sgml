<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook V4.1//EN">
<book><bookinfo>
<title>Geodetic Date Modelling System: Thesis</title>
<authorgroup>

<author>
  <firstname>Daniel</firstname>
  <surname>Fernandez</surname>
  <affiliation><jobtitle>991672</jobtitle></affiliation>
</author>

<author>
  <firstname>Michael</firstname>
  <surname>Still</surname>
  <affiliation><jobtitle>964076</jobtitle></affiliation>
</author>

<author>
  <firstname>Blake</firstname>
  <surname>Swadling</surname>
  <affiliation><jobtitle>982087</jobtitle></affiliation>
</author>

<author>
  <firstname>Kristy</firstname>
  <surname>Van Der Vlist</surname>
  <affiliation><jobtitle>983118</jobtitle></affiliation>
</author>

<author>
  <firstname>Nick</firstname>
  <surname>Wheatstone</surname>
  <affiliation><jobtitle>983131</jobtitle></affiliation>
</author>

</authorgroup>
<abstract><title>Abstract</title>
<para>

This thesis discusses the theory and implementation of the Geodetic Data Modelling System (<command>GDMS</command>), a software 
time series analysis application designed specifically for the analysis of Global Positioning Satellite (GPS), Very Long Baseline
Interferometry (VLBI) and Satellite Laser Ranging (SLR) data. The year long project carried out by the authors, involved first 
prototyping a system in MATLAB, before implementing a full version in ANSI ISO C++. This thesis starts with a brief discussion on 
the history of time series analysis, and an introduction to the <command>GDMS</command>. It is then divided into five main chapters,
the first providing a comprehensive look at the theory behind all process employed in the system. These include time domain and 
frequency domain analysis, along with interpolation, windowing and the user interface. The second chapter
discusses design methodology,  the implementation of the prototype and of the <command>GDMS</command> itself. In addition, issues 
which arose are examined, along with discussions future enhancements that could be made to this system. The third chapter 
discusses the testing methodology, which is an integral part of any systems software design and the fifth chapter discusses the
documentation process. The final chapter concludes the  project by looking back on it both in terms of its measure of success, 
and how we, the project team, have enhanced our knowledge.
</para>
</abstract>
</bookinfo>

<preface><title>Acknowledgments</title>
<para>
Daniel Fernandez would like to thank:- his cat
</para>

<para>
Michael Still would like to thank:- his loving family (Catherine, Andrew, George the Feotus, and his x21)
</para>

<para>
Blake Swadling would like to thank:- Jack Daniels
</para>

<para>
Kristy Van Der Vlist would like to thank:- Glenfidich
</para>

<para>
Nick Wheatstone would like to thank:- Abdoul's Camels
</para>

<para>
The group would like to thank:- Professor Peter Morgan and James Squire
</para>
</preface>

<chapter id="ch01"><title> Introduction </title>
  <sect1><title>Introduction</title>

<para>
Time series analysis of discrete signals is not a new concept. Texts on this topic were printed as early as 1924. It was
not, however, until  the digital computer appeared that the momentum of this field increased dramatically. The FORTRAN programming
language enabled complex mathematics to be written on a computer. Many routines and texts appeared around
this time, with the focus applications being in the areas of oil and space exploration. Further significant texts emerged from
Australia in the nineteen sixties and seventies on Fourier analysis of time series and its applications.
</para>

<para>
Published works such as Numerical Recipes emerged, firstly in FORTRAN, then C and C++. This is accepted as
the standard reference today for Numerical Analysis algorithms, covering least squares analysis, interpolation,
statistics and frequency analysis techniques. Time Series View (TSVIEW) is a more recent example of a computerised
package dealing time series analysis, developed at the Massachusetts Institute of Technology (MIT). It is a front end to MATLAB 
providing time series analysis in the time domain. TSVIEW also provides an intuitive, interactive graphical user interface thereby
allowing the easy analysis of a given data set.
</para>

<para>
MIT is an institution with significant influence in the area of time
series analysis. It is also home to the GAMIT/GLOBK suite of programs for the analysis
of Global Positioning System (GPS), Very Long Baseline Interferometry (VLBI) and Satellite Laser Ranging (SLR) data. 
It is worthwhile to take a brief look at the functionality of this suite, paying particular attention to the CVIEW module. 
Its features include the following:

<orderedlist>
<listitem><para>Selecting or spanning a region of points for closer inspection (i.e. zooming).</para></listitem>
<listitem><para>The ability to display with various symbol, data that is used in the solution. This also involves data that 
does not correctly fit a particular model and is therefore weighted out of the solution.</para></listitem>
<listitem><para>The ability to weight and un-weight points.</para></listitem>
<listitem><para>The ability to changes the value of any number of points.</para></listitem>
<listitem><para>Allows the displacement of elementary statistics and regression coefficients.</para></listitem>
<listitem><para>The testing of different processing strategies.</para></listitem>
<listitem><para>A variable number of display windows open from one to 10.</para></listitem>
<listitem><para>Quick and flexible use of file pointers.</para></listitem>
</orderedlist>

</para>

<para>
  The geodetic data sets used in this analysis are extensive, covering in the order of ten years of measurements from stations 
  around the globe. The GPS, VLBI, and SLR data are susceptible to various conditions, potentially affecting the validity of the data. 
  Some of these conditions include:-
   <itemizedlist>
     <listitem><para>Ionospheric delay.</para></listitem>
     <listitem><para>Temperature fluctuations.</para></listitem>
     <listitem><para>Other considerations including projected satellite orbits.</para></listitem>
   </itemizedlist>
  With the ability to model certain situations, taking into consideration factors such as those above, conclusions can be drawn 
  regarding, amongst other things, tectonic plate movement, the level of continental drift and weather conditions.  
  
</para>

<para>
  
  Whilst both TSVIEW and CVIEW are useful tool for analysis of this type of data, the are not without their short comings. MATLAB
  is expensive to purchase, and this has prohibited some institutions from being able to use TSVIEW in this type of analysis
  which requires this program to run. In addition, TSVIEW uses the MATLAB algorithms to calculate its data transformations and whilst
  this guarantees that all mathematical functionality has been rigorously tested and validated, these routines are not specifically
  optimised for the analysis of geodetic data. Similarly, the CVIEW package is not without its problems. Although it is faster than
  TSVIEW in calculating a solution this package has grown as a set of individual modules that have been added over time, and the whole
  system is preserved inside X Windows wrappers. This has had the effect of making the package itself almost possible to maintain in
  recent times. CVIEW also suffers from the problem that its user interface is dated and is not considered particularly user
  friendly. This has meant that there is a steep leaning curve required for any user that may wish to use this product. One of the
  most important limitations of both these packages is, however, that they both lack any support for frequency domain analysis which
  is imperative for the analysis of this type of data.
</para> 

<para>
  Consequently, the final year project developed by Daniel Fernandez, Michael Still, Blake Swadling, Kristy Van Der Vlist and Nick
  Wheatstone known as The Geodetic Data Modelling System (<command>GDMS</command>) was created for the specific purpose of remedying
  the short falls of the existing geodetic data analysis applications. This system aims to provide and expand upon the features
  included in both TSVIEW and CVIEW. The <command>GDMS</command>  provides time domain analysis in the form of Linear Least Squares Regression
  modelling. In addition, several interpolation and Gaussian filter (windowing) algorithms have been
  implemented, thereby allowing the data to be transformed into the frequency domain for analysis with such tools as Fast Fourier
  Transforms (FFT) and Power Spectral Density Plots (PSD). By allowing analysis in both the time and frequency domains the <command>GDMS</command>
  gives the user greater flexibility in determining the general data trend as well as detect and remove any erroneous data points that
  may appear in the given data set. It also provides an intuitive user interface capable of, among other things graphing,
  spanning and display multiple windows.
</para>

<para>
  This thesis aims to provide a comprehensive insight in the theory behind the processing techniques
  that have been employed in the <command>GDMS</command>. Where appropriate alternative theories and algorithms are presented and 
  compared. The design methodology used in the implementation is discussed, along with the the prototype and
  each specific algorithms with reasons for why they were chosen. Issues that arose throughout this process are also highlighted,
  ranging from design changes during the project, to difficulties with the implementation of various algorithms.
  In each section, we examine the potential improvements and enhancements that could be made to the
  various modules. A background on the testing approach is provided. As projects increase in size, standardised testing 
  procedures becomes paramount. Here we detail the approach used for the testing of the various modules of the project. The thesis 
  closes by providing among other things, what we believe to be a measure of the success of the project, both in terms of the 
  project itself and also in terms of our enhanced knowledge and experience as a direct result of the project. 
</para>

</sect1> <!--backgorund-->


</chapter>

<chapter id="ch02"><title> Theory </title>
  <sect1><title>Time Domain Analysis</title>

<sect2><title>Introduction</title>
  <para>
   Analysis of discrete signals in the time domain is essential to the understanding of geodetic data produced
  by GPS, VLBI and SLR. By applying a linear least squares regression model to the system underlying data trends can be ascertained,
  such as, the average rate of continental drift. Linear regression analysis also allows such phenomenon as Random 
  Walk and white noise, both of which which are known to affect each type of geodetic data to differing degrees,
  to be detected and removed. This, in turn, enables the geodetic community to gain a greater understanding of the elements that may 
  affect the accuracy of a given data set. In addition, an analysis of residuals resulting from a linear regression not only 
  give a measure of how accurately the line of best fit models the data set it can also aid in the detection and deletion of 
  erroneous points.        
  </para>
</sect2> <!--Introduction-->


<sect2><title>Theory</title>
  <sect3><title>Linear Least Squares Regression Modelling</title>
    <para>
    One of the major problems in discrete signal analysis is that these systems are almost always inconsistent, that is, 
    there is no exact solution to the equation of the line 
    
    <equation>
      <title>Equation of the Line</title>
      <graphic format="eps" fileref="gen-fTD-equ1.eps">
    </equation>

    
    where <command>m</command> is the slope and <command>c</command> is the y-intercept. This is usually due to the fact that there
    are more data points in the system than those required to solve this equation. Linear least squares regression modelling is a 
    method used to calculate <command>m</command> and <command>c</command>, such that, it is the closest approximation
    <command>y</command> for the given set of observations in the system, also known as a line of best fit. In the analysis of
    geodetic data the line of best fit is given as the solution to the matrix equation
    
    <equation>
      <title>Linear Least Squares Regression Equation 1a</title>
      <graphic format="eps" fileref="gen-fTD-equ2.eps">
    </equation>
    
    where <command>X</command> is the corrections to the apriori estimates
    
    TODO: NEED TO MAKE CORRECT X MATRIX!
    
    and <command>n</command> is the number of observations in the system. The <command>A</command> matrix is the know as the design
    matrix
    
    <equation>
      <title>Design Matrix</title>
      <graphic format="eps" fileref="gen-fTD-equ4.eps">
    </equation>
    
    <command>P</command> is the <command>n</command> x <command>n</command> weighting matrix and <command>L</command> is the apriori
    matrix 
    
    <equation>
      <title>Apriori Matrix 1a</title>
      <graphic format="eps" fileref="gen-fTD-equ5.eps">
    </equation>
    
    </para>
    
    <para>When carrying out least squares regression analysis on geodetic data it is common to set the initial apriori estimates of
    the slope and y-intercept to zero, thereby making all value of <command>L<subscript>c</subscript></command> = 0. By applying
    this initial constraint the apriori matrix becomes
    
    <equation>
      <title>Apriori Matrix 1b</title>
      <graphic format="eps" fileref="gen-fTD-equ6.eps">
    </equation>
    
    and hence Equation 2 becomes
    
    <equation>
      <title>Linear Least Squares Regression Equation 1b</title>
      <graphic format="eps" fileref="gen-fTD-equ7.eps">
    </equation>

    </para>
        
    <para>Another common constraint use in this type of analysis is to set y-intercept <command>c</command> at the point of the
    first time observation in the given system. This is desirable because of the time format used in geodetic
    data systems, that is, a decimal number where the numerator corresponds to the year and number of days into the year is 
    denominator. For example, the date value 2002.0014 corresponds to the Gregorian calender date of 12:00, 1 January 2002. If 
    x-axis was not constrained, the y-intercept would always be given at <command>x</command> = 0, which corresponds to a date of 
    4000 BC (approximately 500 years before the Bronze age), thus 
    rendering this value virtually meaningless. Therefore, the x-axis is constrained by setting 
    <command>x</command> = <command>t - t<subscript>0</subscript></command> where <command>t<subscript>0</subscript></command> 
    is the first time observation in the given system. This has the effect of making the <command>c</command> value more 
    meaningful and simplifying any graph of this data. 
    </para>
  </sect3><!--Linear Least Squares Regression Modeling-->
  
  <sect3><title>Weighting Matrices - Variance Co-variance and Random Walk</title>
    <para>It is not uncommon in geodetic data, especially those taken over a long period of time (i.e. a number of years) that some
    of the data value are less reliable than others. This can be due to failure or replacement of data gathering 
    equipment or natural phenomena such as earthquakes. In addition, random or white noise is known to
    affect different types of geodetic data to varying degrees. This type of signal distortion is truly random and has a zero mean
    (<command>&mgr</command>) and a variance (<command>&sgr</command>), such that, over a sufficently large data set its
    effects are cancelled out. The problem occors, however, where smaller systems are used, the white noise is not cancelled 
    out and can distort the mean value of the given system. It is important, therefore, to have some mechanism of ensuring 
    that this, potentially erroneous data, does not influence the linear regression analysis in an adverse manner. 
    In order to do this what is known as a Variance Co-variance (VCV) weighting matrix is employed. 
    </para>
    
    <para>A VCV weighting matrix is a strictly diagonal matrix where correlation is not admitted, that is 
    <command>P<subscript>ij</subscript></command> = 0 where <command>i</command> &ne; <command>j</command> 
    which models a standard Gaussian distribution (also known as a bell curve).
    
    <equation>
      <title>Standard Gaussian Distribution</title>
      <graphic format="eps" fileref="gen-fTD-equ10.eps">
    </equation>
     
    This method works by pre-multiplying the observation values with a number that represents its 
    "correctness" (see EQUATION 2). Therefore the weighting value specified on the diagonal of the VCV matrix should be a 
    percentage value ranging from one to zero, whereby a value of one one means the given data point fully participates 
    in the calculation and a value of zero indicates that the data point was erroneous and is discarded. 
    </para>
    
    <para>Due to the nature of method of weighting data values, great care must be taken so that the linear regression model
    calculated is not distorted. This is due to the fact that, by pre-multiplying the observation data with a weighting matrix the
    their values are essentially being changed. If too few erroneous data points are un-weighted, the least squares line calculated 
    will not accurately represent the general data trend. Similarly, weighting out too may data points will cause the
    same error to occur. For example, one method of weighting an inconsistent system would be to assign a weighting value 
    according to standard deviation, that is, for all points within standard deviation above or below the mean would be given a 
    weight of 1. Similarly, points within two standard deviations, but greater than one standard deviation would be weighted 0.75 
    and any point two standard deviations above or below the mean would be given a weight of 0.5. Finally, all other points would 
    be weighted to zero. This model, however, has been known to unnaturally distort the data when applied to geodetic systems. 
    This is due to the fact that the data maybe skewed, that is where the third moment is 
    
    EQUATION SKEWED
    
    and/or affected by kurtosis where the forth moment is
    
    EQUATION KURTOSIS
    
    By applying this model, these anomalies will only be accentuated in the resulting linear regression model and either too many
    data points will be weighted less than zero (in the case of skewed data) or too few points will be weighted out (in the case of 
    data affected  by kurtosis). A far safer method of assigning a weighting matrix is to simply weight any data point greater 
    than three standard deviations above or below the mean to zero and all other values as one. This ensures the resulting model
    will be calculated using a standard Gaussian distribution, and thus will be unaffected by these phenomenon.
    </para>
    
    <para>Another use of weighting matrices is to remove any underlying noise that may be affecting the system. One such
    trend is known as Random Walk, which is a phenomenon that the geodetic community believes occurs, particularly in GPS data.
    Detection of this trend become more urgent in recent times because it is believed that Random Walk can only be accurately seen 
    in systems that span at least six years, or preferably 10 years and data sets of this length have not become available until
    now. If it is assumed that the data follows a standard Gaussian distribution (SEE FIGURE GAUSSIAN DISTRIBUTION) that is, 
    the observation values are evenly distributed then the expected Random Walk values are 
    </para>    

    <table frame='all' shortentry='0' toCEntry='1' >
    <title>Table Of Random Walk Values</title>
    <TITLEABBREV>Table of Random Walk Values</TITLEABBREV>
    <tgroup cols='5' align='left' colsep='1' rowsep='1'>
    <thead>
      <row>
        <entry>Time</entry>
        <entry>1</entry>
        <entry>2</entry>
        <entry>...</entry>
        <entry>n</entry>
      </row>
    </thead>
    <tbody>
    <row>
      <entry><command>RW</command></entry>
      <entry>R<subscript>1</subscript></entry>
      <entry>R<subscript>1</subscript> + R<subscript>2</subscript></entry>
      <entry>...</entry>
      <entry>R<subscript>1</subscript> + R<subscript>2</subscript> + ... + R<subscript>n</subscript></entry>
    </row>
    </tbody>
    </tgroup>
    </table>

    <para>
    and thus the weighting matrix is
    </para>

    <equation>
      <title>Random Walk Weighting Matrix</title>
      <graphic format="eps" fileref="gen-fTD-equ12.eps">
    </equation>
    
    <para>
    It is anticipated that using such a weighting matrix will remove any underlying Random Walk trend that may be affecting the data. 
    At this time, however, this is some debate as to which equation should be use in order to determine the random walk co-efficients 
    and thus the optimal weighting matrix. As a result, there is limited support for this type of calculation within the scope of 
    the project.
    </para> 
  </sect3><!--Weighting Matrices - Variance Co-variance and Random Walk-->
  
  <sect3><title>Residuals and Determining VCV Weighting Models</title>
    <para>Residual space is another important aspect of Time Domain analysis. In essence, a residual is vector that represents the
    distance between the observed value and the least squares regression line of best fit. The residual vectors are therefore
    calculated by the matrix equation
    
    <equation>
      <title>Residual Equation</title>
      <graphic format="eps" fileref="gen-fTD-equ13.eps">
    </equation>
    
    Where <command>L</command> is specified by (EQUATION 5). If the same constraint is applied to the apriori matrix as was used in
    the linear least squares model, that is, it is assumed that or initial model has a slope and y-intercept of 0, the residual
    equation becomes
    
    <equation>
      <title>Constrained Residual Equation</title>
      <graphic format="eps" fileref="gen-fTD-equ14.eps">
    </equation>
    
    </para>
    
    <para>At the very least, the residual matrix shows how well the regression line approximates the given system, that is, the 
    smaller the values of <command>V</command> the better the fit of the model. In addition, if the residuals are transformed into 
    the Frequency Domain via a Fast Fourier Transform, an optimal model would give a totally flat frequency response. 
    Another property of residuals is that the sum of the residuals should always be equal to zero. This is a very useful 
    property for testing the validity of any residual calculation.
    </para>
    
    <para>Calculating residuals are also essential for determining the VCV weighting model that should be applied to the given 
    system. As it has been seen, calculating an optimal VCV weighting model relies heavily on determining the standard 
    deviation of a system according to a Gaussian distribution. One method achieving this is to calculate the new weighting 
    matrix as
    
    <equation>
      <title>Weighting Matrix Equation 1a</title>
      <graphic format="eps" fileref="gen-fTD-equ16.eps">
    </equation>
    
    where <command>N</command> is given by
    
    <equation>
      <title>Weighting Matrix Equation 1b</title>
      <graphic format="eps" fileref="gen-fTD-equ17.eps">
    </equation>
    
    This method, however, leads to the new weighting matrix <command>&Sgr</command> that is not strictly diagonal and this is
    undesirable as it is computationally much slower to calculate. For example, if the matrix equation
    <command>A<superscript>T</superscript>P</command>  
    where <command>A</command> was a <command>n</command> x 2 matrix and <command>P</command> was a
    <command>n</command> x <command>n</command> weighting matrix. If the weighting matrix was strictly
    diagonal then then it would take  2 x <command>n</command> multiplication operations to compute. 
    Conversely, if <command>P</command> was a non-diagonal matrix then it would take 2 x
    <command>n</command><superscript>2</superscript> and 2 x
    <command>n</command><superscript>2</superscript> addition operations to compute the same equation.   
    </para>
    
    <para>Considering the slowness of using a weighting matrix that is not diagonal, another method of calculating a weighting
    matrix was sought. This method works entirely in the residual space and ensures that the weighting matrix is always diagonal.
    In this calculation, a line of best fit is calculated using (EQUATION 7) and the residuals are calculated using (EQUATION 14). 
    The residual matrix is then sorted in ascending order. From there the median and interquartile range can be calculated. The
    threshold for three standard deviations above or below the mean, assuming a standard Gaussian distribution can be calculated by
    adding 1.5 x the interquartile range to the 75% limit and subtracting 1.5 x the interquartile range from the 25% limit, 
    resulting in a box and whiskers plot. 
    
    <figure><title>Box Plot of Data Set Re-weighting Using Resdiuals</title>
        <graphic format="eps" fileref="gen-TD-boxplot.eps">
    </figure>

    Therefore, any residual with a value greater than three standard deviations above the mean is considered to be erroneous and 
    will be weighted out.
    
    </para>
    
     
  </sect3><!--Residuals and Detetermining VCV Weighting Models-->
</sect2> <!--Theory-->

<sect2><title>Conclusion</title>
  <para>Time domain analysis of discrete geodetic signals is a powerful tool in determining data trends. Linear Regression Modelling
  can be employed to establish any general patterns in the data and give an indication of any data anomalies. It has also been seen 
  that by using a weighting matrix, erroneous data can be removed, thereby allowing the most accurate regression model to be fitted. 
  Weighting matrices can also be employed to detect and remove any underlying data trends, such as Random Walk that may be adversly
  affecting the system. Time Domain analysis also allows a given system to be modelled in the residual space. This is important for 
  may reasons, firstly, the residuals give an indication of how well the regression model actually fits the observation data in both 
  the time and frequency domains. In addition, the residual space allows an optimal VCV weighting matrix to be calculated, such 
  that it is always guaranteed to be strictly diagonal. This vastly reduces the computational time required to calculate a 
  least squares solution.     
  </para>
</sect2> <!--Conclusion-->

</sect1> <!--TDA-->
  <sect1><title>Interpolation</title>

<sect2><title>Introduction</title>
  <para>One problem with the GPS datasets is that there are often gaps in the data.
  Missing data points skew data results and prevent transformation into the frequency
  domain. Another problem is that the time scales in GPS data sets
  are often non-linear.  Interpolation solves these problems by filling the gaps in the data.
  A large number of interpolation methods exists, all having their advantages and disadvantages for
  different applications. Disucssed in the following section are six iterpolation methods that have been included in
  this project.
  </para>
</sect2> <!--Introduction-->

<sect2><title>Theory</title>
  <para>
  Theoretically, each of the datasets that we analyse are sampled once a day at
  exactly twelve o'clock. This is however, not always possible for reasons such as 
  such as equipment failure, extreme weather and in some case local politics also play a role.
  </para>
  <para>
  The Fourier transform algorithms available to us require that the dataset be regular.
  This means that if a dataset is missing a point or has an irregular sample rate then
  it can't be transferred to the frequency domain.  The dataset has to be made regular
  either by removing points or interpolating new points.
  </para>
  <para>
  This irregular sampling rate has been compounded by having the datasets stored on a
  non-linear time scale.  The data that we have been provided has been sampled regularly
  once a day at noon as timed by an atomic clock; so the sample rate is pretty regular.
  However the sample date/time has then been stored in decimal year format to 8 significant
  figures.  Ignoring machine rounding errors this creates a problem.  During a regular
  year one day is 0.00273972.  However one in every four years is a leap year and the
  length of a day is 0.00273224, a smaller number.  Also at the start of each year a
  rounding occurs so that the first reading of the year occurs at XXXX.0014.  This
  rounding creates the illusion different time distance again to be between the last
  day of one year and the first day of the next.  In short before interpolation can
  occur a new time scale independent of calendar years has to be built.
  </para>
  <para>
  The new time scale that we convert to is what is known as a truncated Julian day.
  The Julian day system uses an integer day count since the first of January 4714 BC.
  However because this produces extremely large numbers which could potentially cause
  loss of accuracy due to machine limitations.  To prevent this we have used the first
  of January 1901 as our start date.  The existing decimal dates then converted to
  truncated Julian day and rounded to the nearest whole number.  Once the timescale
  is linear the data can be interpolated without fear of loss.
  </para>
  <para>
  Different interpolation methods have different strengths and weaknesses.  On an
  arbitrary dataset it is impossible to tell which interpolation method will be
  the most accurate.  To this end six different interpolation methods have been
  provided, each will be outlined below on its operation, strengths and weaknesses.
  </para>

  <sect3><title>Nearest Neighbour Interpolation</title>
  <para>
    The simplest interpolation method provided. Linear interpolation
    simply set the value of any new point to the value of the nearest point on the
    original dataset.  The advantage of this is that any added points will be of
    the same approximate value as nearby points.  There are a number of disadvantages
    with nearest neighbour approximation.  Firstly any new points will not follow
    any linear or frequency trends in the data, this could lead to inaccuracy of
    models both in the time and frequency domains.  Also if new points lie close to
    outliers then very inaccurate point may be generated
  </para>
  </sect3>

  <sect3><title>Linear Interpolation</title>
  <para>
    Nearest neighbour interpolation is widely used because it is still simple yet
    generally produces better results than nearest neighbour.  To do nearest neighbour
    interpolation each new point is placed on a line between the two adjacent points.
    The Equation for adding a new point is: (TODO:ADD LINEAR INTERP EQUATION 1)
  </para>

  <para>
    where: (TODO:ADD LINEAR INTERP EQUATION 2)
  </para>
  <para>
    Linear interpolation has several advantages.  Firstly each added point has a
    value that is approximately the same as nearby value; this means badly out of
    range value are very rare.  Secondly the added point will follow any local
    linear trends between the two points.
  </para>
  <para>
    The disadvantages of linear interpolation stem from the interpolation method only working
    with the two adjacent points. If the line between these two points does not follow
    the overall trend of the data then loss of accuracy occurs.
  </para>
  </sect3>

   <sect3><title>Cubic Spline Interpolation</title>
  <para>
    This is a one of the many implementations of cubic spline interpolation. It has been given the
    generic name as no name was available for this particular implementation.  The only
    difference between this interpolation type and a natural spline is the choice of
    second derivatives used at the end points.  With this implementation the values of the
    second derivative for the end points are set the same as the second derivatives of the
    second to end points.  Documented commentary says that this type of spline curves a
    little bit too much at the ends.
  </para>
  <para>
    The maths used to generate these cubic spline is very similar to producing a natural
    spline.    To produce a spine for a dataset of n+1 points you have to produce n separate
    cubics.  Each of these cubic should have each end match up exactly with the
    points to either side.  And on the same point the two adjacent cubic should
    have shared first and second derivatives.
  </para>
  <para>
    Each of these cubics can be described by the equation: (TODO: Spline equation 1)
  </para>
  <para>
    Because each of these cubics begins at a any existing point we can say: (TODO: Spline equation 2)
  </para>
  <para>
    Let h<subscript>i</subscript>=(x<subscript>i+1</subscript> - x<subscript>i</subscript>),
    be the width of the the interval.
  </para>
  <para>
    Also if we make s the set of second derivatives; then through algebraic simplification
    we can get: (TODO: Spline equations 3,4,5)
  </para>
  <para>
    With natural splines solving for S is done by solving:(TODO: Spline equation 7)
  </para>
  <para>
    The two end values, S<subscript>0</subscript> and S<subscript>n</subscript>, are then
    set to equal S<subscript>1</subscript> and S<subscript>n</subscript> respectively.
  </para>
  <para>
    A full mathematical derivation can be found in Gerald &amp; Wheatley (1999), page 238.
  </para>
  <para>
    The advantages and disadvantages of this cubic spline interpolation are basically the
    same as for natural splines. Splines can provide extremely accurate results when the
    original sample rate is notable greater than the frequency of fluctuation in the data.
    However if the sample rate is less than half the frequency of change (including signal
    noise) in the data then the results can be erratic.  Splines also have a problem
    when it comes to large gaps in the dataset. Because the gap between two points is
    represented by a cubic, large gaps result in peaks or troughs in the dataset.  The
    final problem with spline is that the end sections are inaccurate due to arbitrary
    methods used to assign the derivative of the end points. It is because of this reason
    that two cubic spline methods have been provided for this application.
  </para>
  </sect3>


  <sect3><title>Natural Spline Interpolation</title>
  <para>
    Natural splines are a type of cubic spline, originating from the idea in ancient times when draftsmen and builders 
    would create a smooth curve by pegging a flexible piece of wood between a number of points. Cubic splines are a method
    of recreating this wooden curve with mathematics.  With all cubic splines the
    interpolation is done by inserting a cubic between each two adjacent points.
    At the linking points these cubics must have the same first and second
    derivatives.  Equalizing the derivatives has the effect of making the resulting
    interpolation appear smooth and visually pleasing.  For this reason splines
    are often used in graphics.
  </para>
  <para>
    In the mathematical process of building splines it is impossible to infer the
    derivatives of the two end points of a dataset.  It is the values assigned to
    these end points that determine the type of spine.  In a natural spline the
    end points are set to 0.  This gives a result similar to the ancient wooden
    splines used by builders.  Mathematically this produces a result where the
    end segments are a bit too straight.
  </para>
  <para>
    To produce a spine for a dataset of n+1 points you have to produce n separate
    cubics.  Each of these cubic should have each end match up exactly with the
    points to either side.  And on the same point the two adjacent cubic should
    have shared first and second derivatives.
  </para>
  <para>
    Each of these cubics can be described by the equation: (TODO: Spline equation 1)
  </para>
  <para>
    Because each of these cubics begins at a point we can say: (TODO: Spline equation 2)
  </para>
  <para>
    Let h<subscript>i</subscript>=(x<subscript>i+1</subscript> - x<subscript>i</subscript>),
    be the width of the the interval.
  </para>
  <para>
    Also if we make s the set of second derivatives; then through algebraic simplification
    we can get: (TODO: Spline equations 3,4,5)
  </para>
  <para>
    With natural splines solving for S is done by solving:(TODO: Spline equation 6)
  </para>
  <para>
    The two end values, S<subscript>0</subscript> and S<subscript>n</subscript>, are then
    set to zero.
  </para>
  <para>
    A full mathematical derivation can be found in Gerald &amp; Wheatley (1999), page 238.
  </para>
  <para>
    Because of the smooth nature of the interpolation curve produced; splines can provide
    extremely accurate results when the original sample rate is notable greater than
    the frequency of fluctuation in the data.  However if the sample rate is less than half the
    frequency of change (including signal noise) in the data then the results can be
    erratic.  Splines also have a problem when it comes to large gaps in the dataset.
    Because the gap between two points is represented by a cubic, large gaps result
    in peaks or troughs in the dataset.
  </para>
  </sect3>
  
  <sect3><title>Newton Divided Difference Interpolation</title>
  <para>
    Mathematicians have long know that a nth order difference table can be used to accurately
    recreate a polynomial of degree n.  Newton divided differences uses a similar method to
    approximate new points within an tributary dataset.
  </para>
  <para>
    Newton divided differences is based on building a table of divided differences.  The
    divided difference between two points in a dataset is defined as: (TODO: divided difference eq 1)
  </para>
  <para>
    Likewise higher order divided differences can be defined as: (TODO: divided difference eq 2)
  </para>
  <para>
    These divided differences are built into a table as follows: (TODO: divided difference eq 3)
  </para>
  <para>
    This table can then be used to generate an equation: (TODO: divided difference eq 4)
  </para>
  <para>
    A full mathematical derivation can be found in Gerald &amp; Wheatley (1999), page 229.
  </para>
  <para>
    Because divided differences estimates a curve by using polynomial, using a polynomial
    of too low or high a degree can result in errors. The accuracy available by using the
    double precision floating point format also creates a limit.
  </para>
  <para>
    To detect and limit the errors caused by these factors the program calculates error
    estimates. As the divided difference table grows it calculates the rough error for
    a table of size one less.  If the error estimate has shrunk then the program continues
    building the table to a higher order.  If the error has grown then the program stops
    building the table.
  </para>
  <para>
    The error for a divided difference of a given order is approximately equal to the change
    that will be incurred by increasing the order of the divided difference table by one.
    So for an order n newton divided difference interpolation the mathematical definition
    of the error is:
  </para>
  <para>
    A full mathematical derivation can be found in Gerald &amp; Wheatley (1999), page 229.
  </para>
  </sect3>
</sect2> <!--Theory-->

<sect2><title>Conclusion</title>
  <para></para>
</sect2> <!--Conclusion-->


</sect1> <!--Interpolation-->

  <sect1><title>Windowing</title>

<sect2><title>Introduction</title>
  <para>Windowing is the breaking of an infinte signal into finite pieces. Since we are not able to record and infinite signal, and
  must analyse a finite sample, it is unavoidable that we perform this process. Truncation of the signal in this way has side effects,
  primarily the introduction of ripples in the frequency domain. These ripples show up as high frequency noise and give</para>
</sect2> <!--Introduction-->

<sect2><title>Theory</title>
 <para>
  As previously stated, truncation of the signal in the time domain leads to unwanted ripples being produced in the frequency domain.
  The basic idea behind windowing the data prior to performing a fourier tranformation is that we can apply a weighting function
  to the sample in order to reduce the introduction of unwanted frequencies. This reduction is a trade off, and the cost is an
  increase in the width of the spectral peak and leakage of energy away from the true frequency to the side lobe skirts.
  This adversley effects the resolution with which we can observe the signal. We must strike a balance between spectral peak width
  and the degree of high frequency attenuation. Ideally we would like to have no skirts, but in practice the best we can do is try to
  control them. The available windowing algorithms vary in their ability to control this while trying to retain a fine specral peak.
  The attributes of each algorithm can be summarised as follows:
  </para>
  <itemizedlist>
  <listitem><para><command>Hanning:</command></para>
     <para>
       This algorithm provides good resolution of spectral peaks and good rejection of side lobe skirts.
       It is a good choice for most applications.
     </para>
  </listitem>

  <listitem><para><command>Hamming</command></para>
    <para>
      Hamming proveds finer resolution of spectral peaks than Hanning, however side lobe skirts are not controlled as well as with Hanning.
    </para>
  </listitem>

  <listitem><para><command>Blackman</command></para>
    <para>
      Peak resolution is not as fine as Hanning, but the response shape flares out less at lower levels and rejection of side lobe skirts
      is better.
    </para>
  </listitem>

  <listitem><para><command>Dolph-Chebyshev</command></para>
    <para>
      Very similar to Hamming, but with with better high frequency attenuation. Where Hamming can give around 50dB attenuation
      for frequencies above the sampling frequency, Dolph-Chebyshev can provide approximately 63dB attenuation for the same frequencies.
    </para>
  </listitem>
  </itemizedlist>
</sect2> <!--Theory-->

<sect2><title>Conclusion</title>
  <para>
    Windowing can be an effective tool in removing the undesirable effects of the Discrete Fourier Transform. Prudent use of a
    suitable algorithm removes the high frequencies introduced via aliasing when the the signal is truncated. Knowledge of the
    attributes and limitations of the individual windowing algorithms, and the judicious selection of the correct one, will assist
    the user in attaining optimal results from the application of the DFT.
  </para>
</sect2> <!--Conclusion-->

</sect1> <!--Windowing-->
  <sect1><title>Frequency Domain Analysis</title>

<sect2><title>Introduction</title>

<para>
  Fourier transforms enable us to view the frequency representation of data that exists in the time domain. This provides 
  a means of detecting erroneous frequencies in the given data set and thus detect noise. Whilst in the frequency domain,
  we can also produce Power Spectral Density (PSD) plots. Once erroneous data has been removed in the frequency domain,
  an inverse transform can be applied to return the data to the time domain. Here we ascertain the the effects of removing such data
  had on the original system. 

</para>

</sect2> <!--Introduction-->



<sect2><title>Theory</title>

<para>
  The concepts of frequency domain analysis should commence with the Fourier transform. Any waveform can be constructed purely 
  from a set of sine and cosine waves. An example of this is a pure square wave, which can be constructed using only the odd 
  harmonics of a sin wave as follows:
  
  <equation>
    <title>Square Wave</title>
    <alt>x(n) = sin(f) + 1/3*sin(3f) + 1/5*(sin5f)</alt>
      </equation>
	
  for all odd harmonics. Where <command>f</command> is the fundamental frequency.
  In addition, just as any signal can be modelled by a set of sine and cosine waves the converse is also true and any signal can be
  decomposed into a set of sine and cosine waves. This is achieved by using the Fourier Transform, which preforms this signal 
  decomposition and transforms it into the frequency domain. The Fourier Transform is calculated as

  <equation>
    <title>Fourier Transform (Rabiner and Schafer, 1978)</title>
      </equation>
  
  where <command>&egr;<superscript>j&ohgr;</superscript></command> is equal to <command>&ohgr;</command> or the angular frequency.
  We can also computer the inverse Fourier transform, returning us a function of time

  <equation>
    <title>Inverse Fourier Transform (Rabiner and Schafer, 1978)</title>
      </equation>
</para>

<para>  
  The above transforms are not, however, available for all sequences <command>x</command>(<command>n</command>). This is due to the
  fact that the basic Fourier transform is not reliable for signal sampled at discrete time 
  interval as it is designed for use with with continuous signals (Lepple 2000). In addition, this set of equations is not guaranteed
  to converge for all  <command>x</command>(<command>n</command>). To remedy this, the Discrete Fourier transform (DFT), is often
  used thus enabling the transformation of a discrete signal and guaranteeing convergence. This is achieved restricting the 
  sequence <command>x</command>(<command>n</command>) to the following conditions:
</para>

<itemizedlist>
<listitem><para> The <command>N</command> values <command>x</command>(0)..<command>x</command>(<command>N</command>-1).</para></listitem>
<listitem><para> Equidistant values around the unit circle.</para></listitem>
</itemizedlist>

<para>
 The formula for the DFT is

  <equation>
    <title>Forward Discrete Fourier Transform (Rabiner and Schafer, 1978)</title>
      </equation>

  and the inverse DFT is given by
</para>
  <equation>
    <title>Inverse Discrete Fourier Transform (Rabiner and Schafer, 1978)</title>
      </equation>
<para>
  The discrete frequency <command>k</command> above is given by 
  <command>f</command>(<command>k</command>) =
  <command>k</command>*<command>f<subscript>s</subscript></command>/<command>N</command> 
  where <command>f<subscript>s</subscript></command> is the sampling frequency of the signal.
</para>

<para>
  The problem with the DFT is that it is computationally expensive, with O(<command>N<superscript>2</superscript></command>). 
  In recognition of this limitation the Fast Fourier Transform (FFT) was developed.
  This function takes into account the fact that in calculating a DFT much time is wasted with unnecessary multiplications. 
  For example, it is unnecessary to multiply any number by zero, as the answer will always be zero. 
  Given that multiplication is one of the slowest operations on most types of computers, reducing number that need to be calculated
  will shorten the time taken to calculate a solution. 
  The FFT also decreases the computational load of a DFT by taking into account the fact that a sinusoid is
  periodic, and its values are mirrored around the x-axis at <command>T</command>/2. For example, every 90 degrees, the value 
  one reappears in a sine or cosine signal, changing only its sign. Many results, therefore, may determined by taking into account 
  these factors. Through this increased efficiency, the FFT reduces the computation time to
  O(<command>N</command>log<command>N</command>).
</para>

<para>
  The power spectrum is very useful in signal processing. It is the most common frequency measurement, allowing you to determine the
  amount of energy that exists at a given frequency. Derived from the power spectrum is the power spectral density (PSD) 
  measurement. A PSD plot provides information on the magnitude of energy that exists in a given band of frequencies. 
  This technique is particularly useful for measuring the noise content in a signal. A PSD plot is created by plotting the 
  magnitude of the real and imaginary components of the FFT results. A good illustration of this concept is that of a 
  sine wave with its corresponding PSD plot.

  <figure><title>50 kHz sin wave</title>
      <graphic format="eps" fileref="gen-50HzSin.eps">
  </figure>

  <figure><title>PSD plot of a 50 kHz sin wave</title>
      <graphic format="eps" fileref="gen-fft50HzSin.eps">
  </figure>

  In the above figures, we see a pure sin wave with a frequency of 50Hz. This is confirmed in the PSD plot, where we see
  that sin wave has energy only at 50Hz.

</para>

<para>
  There are some constraints which are implied when carrying frequency analysis.

  <itemizedlist>
  <listitem><para>Maximum frequency analysed.</para></listitem>
  <listitem><para>Frequency resolution.</para></listitem>
  </itemizedlist>

  The maximum frequency analysed relates to the sampling theorem, also know as the Nyquist theorem. The theorem effectively states:
</para>

<para>
  Given a signal <command>x</command>(<command>n</command>) sampled at a sampling frequency of
  <command>f<subscript>s</subscript></command>, the maximum frequency that can be analysed,
  <command>f<subscript>max</subscript></command> is equal to 1/2 of <command>f<subscript>s</subscript></command>. 
  In other words, to accurately represent and analyse the
  signal <command>x</command>(<command>n</command>), it must be sampled at a rate greater the
  2<command>f<subscript>s</subscript></command>, where 2<command>f<subscript>s</subscript></command> =
  <command>f<subscript>N</subscript></command> and <command>f<subscript>N</subscript></command> is known as the Nyquist frequency. 
  The simplest example to illustrate this is the sampling of a 20kHz audio signal. To accurately represent this signal,
  it must be sampled at greater than 40kHz. This value is typically 44100Hz, for example CD audio. Generally signals are sampled
  at several times <command>f<subscript>N</subscript></command> to ensure accurate results. In the case of
  time series analysis, where datasets are typically sampled at a rate of once per day, the maximum frequency that can be analysed 
  is 0.5 cycles/day.
</para>

<para>
  As mentioned earlier, noise is inherent in the datasets with which we are dealing. Once common example is Random Walk.
  This type of noise changes with time and therefore eludes accurate prediction. For further details on Random Walk, please refer
  to the time domain analysis chapter of this thesis. A convenient and appropriate method for viewing Random Walk, in
  particular the frequencies at which it is most prominent, is in the frequency domain. The conversion to the frequency
  domain is commonly done from the residual space of a dataset, i.e. the dataset has already undergone least squares regression
  analysis, including calculation of the residuals. Many of the datasets contain what appear to be Random Walk. One such example
  is the dataset from CEDUNA. In the residual space(see figure below), the periodicity of the signal is clear.
</para>

<figure><title>CEDUNA_GPS(UP) - Residuals</title>
<graphic format="eps" fileref="gen-gdms-cedu-res.eps">
</figure>

<para>
  In the frequency domain, we can now expect to see which frequency components make up the above signal. In particular,
  we want to identify the noise component of the signal. The following figure is the result of an FFT on the Residuals
  of CEDUNA (UP).
</para>

<figure><title>CEDUNA_GPS(UP) - PSD</title>
<graphic format="eps" fileref="gen-gdms-cedu-up-FFT.eps">
</figure>

<para>
  So we see energy at the low frequency end from what seems to be significant value. This first major portion of energy
  is Random Walk. In fact, it is believed that t In some cases, white noise can also be detected through spikes in the frequency domain. 
  It is also believed that Random Walk can be seen through the spectrum provided on the graph.
</para>

<para>
  The frequency resolution of a signal is also constrained by the fact that it is inversely proportional
  to the length of the waveform. Subsequently, if the length of the analysed waveform is doubled, the resolution is halved.
  This is clearer if we consider the relationship <command>f<subscript>res</subscript></command> =
  <command>f<subscript>s</subscript>/N</command>, where <command>N</command> is number of points (IN WHAT!?!). If we double the
  number of points while maintain the sample rate, our frequency resolution is halved.
</para>

</sect2> <!--Theory-->


<sect2><title>Conclusion</title>
  <para>
    There is no question as to the value and importance of frequency domain analysis of time series data. We are able to see
    information invisible to us in the time domain. Trends in the data relating to frequency can be identified,
    and it is even possible to reduce noise by removing
    points while in the frequency domain before returning to the time domain. TODO: more more more...
  </para>
</sect2> <!--Conclusion-->

</sect1> <!--FDA-->

  <sect1><title>User Interface</title>

<sect2><title>Introduction</title>
  <para></para>
</sect2> <!--Introduction-->

<sect2><title>Theory</title>
  <para></para>
</sect2> <!--Theory-->

<sect2><title>Conclusion</title>
  <para></para>
</sect2> <!--Conclusion-->

</sect1> <!--UI-->

</chapter>

<chapter id="ch03"><title>Implementation</title>

  <sect1><title>Design Philosophy</title>

<sect2><title>Introduction</title>

<para>
The first design requirement for the <command>GDMS</command> project, was that it be licensed under the GNU General Public
License. The reason for this is to allow access to the software by people who are not necessarily in a
position to use proprietary software. The <command>GDMS</command> package is complex and relies on a number of external
libraries for mathematical and graphical functionality. Similar proprietary libraries can be very
expensive.
</para>

<para>
The second issue here, which is in fact related to the first, is the need to have a package which is
as platform independent as possible. This contributed significantly to the choice of programming
language. In using  ANSI ISO compliant C++, the <command>GDMS</command> package is intended to run on most Unix based
operating systems. It should also be noted that most modern windowing toolkits require C++.
</para>

<para>
The choice of user interface toolkit also maximized the cross platform capabilities of the application. This is because <command>wxWindows</command> has been specifically built to be easily ported, and currently runs on the following platforms:
</para>

<itemizedlist>
<listitem><para>Microsoft Windows</para></listitem>
<listitem><para>Unix (including Linux)/GTK+</para></listitem>
<listitem><para>Unix (including Linux)/Motif and X11</para></listitem>
<listitem><para>MacOS</para></listitem>
<listitem><para>OS/2</para></listitem>
</itemizedlist>

<para>
The next important design decision involves the methodologies used to design implement the <command>GDMS</command>
package. Given the complexity of the system, the Object Oriented approach was deemed the most
suitable. Using OO, the package lends itself well to future enhancements, maintenance and
extensibility. In fact, throughout the implementation process, changes in design were required to
suit the requirements of the growing system. These changes, although not always trivial, were facilitate by
the use of OO methodology.
</para>


</sect2> <!--Introduction-->

<sect2><title>Theory</title>

  <para>
    - ANSI ISO stuff
  </para>

  <para>
    - Serious OO talk
   </para>

</sect2> <!--Theory-->

<sect2><title>Future Enhancements</title>
  <!--may want to integrate this with conclusion?-->
  <para></para>
</sect2> <!--Future Enahncements-->

<sect2><title>Conclusion</title>
  <para></para>
</sect2> <!--Conclusion-->

</sect1> <!--design-->

  <sect1><title>Prototype</title>

  <sect2><title>Introduction</title>

    <para>
      The MATLAB prototype was the minimum requirement of the project. It was to carry out all of the essential
      mathematical functionality providing not only a working model, however also acting as a test bed for final
      product functionality. Most of the features implemented in C++ were tested in this way to ensure correct and
      consistent results. Limited time was spent on dressing the prototype as it was completed early enough in the
      project for group to be able turn to the full implementation in C++.
    </para>

  </sect2> <!--Introduction-->

  <sect2><title>Theory</title>
    <para>
	IDEAS:::

       Prototyping can be an effective technique for identifying requirements in projects and this was no different in
       this case.

       The prototype was originally intended to be a product comparable to TSVIEW, however with addtional functionality,
       such as intrerpolation, windowing and frequency domain analysis.

       Once the the essesntial mathematical functionality in the prototype was complete, work began on a user interface.
       Not long into the UI development, it was decided that it would not be of any benifit and that our attention should
       be focused on the main implementaton of the application.

	Research also - TSView etc..
    </para>
  </sect2> <!--Theory-->

  <sect2><title>Conclusion</title>
    <para></para>
  </sect2> <!--Conclusion-->

</sect1> <!--Proto-->


  <sect1><title>Implementation Issues</title>
    <!--these sections will sit under sect1 - Implementation Issues-->

<sect2><title>Time Domain Analysis</title>

  <sect3><title>Introduction</title>
    <para>The <command>GDMS</command> offers three different types of Time Domain Analysis; VCV Least Squares Regression modelling,
    VCV Least Squares Regression modelling with automated re-estimation and Random Walk Least Squares. VCV regression modelling
    allows a linear model to be fitted to the data in a fast and precise manner, enabling any general data trends to be seen. 
    It also allows the user to detect and remove any erroneous data, thus allowing the most optimal model to be
    calculated. In addition, an automated re-estimation model offers the capability of automatically detecting and removing erroneous
    data points, thereby computing the line of best fit that most accurately models the data. The third method of Time Domain
    Analysis is the Random Walk Least Squares model, which allows a weighting matrix to be specified that detects this underlying
    trend that may be occurring and affecting the accuracy of the data. 
    </para>
  </sect3> <!--Introduction-->

  <sect3><title>Implementation</title>
    <para>Time Domain Analysis is implemented as a separate class inside the <command>GDMS</command> called <command>cepLs</command>
    and all calculations are encapsulated therein. A given solution is essentially reached by solving the Least Squares equation 
    (TD THEORY EQU 7) and a set of residuals are calculated by (TD THEORY EQU 14). Each calculation produces three 
    matrices, <command>X</command> which is the solution to the Least Squares matrix equation and contains the unknown co-efficients 
    <command>m</command> and <command>c</command> of the line equation (TD THEORY EQU 1). The second matrix indicates whether 
    or not the given data point was weighted out of the calculations and finally, the matrix of residuals is also produced.
    </para>

    <para>The first method of Time Domain Analysis offered by the <command>GDMS</command> is known as VCV Least Squares Regression
    modelling. This method allows the user to specify a VCV weighting matrix to be used in the calculations, thus giving the user
    the maximum amount of flexibility in determining to what degree a given point in the data set will influence the line of best
    fit. Another method of VCV analysis offered is that of automatic re-weighting. In this type of analysis the most accurate
    regression model is calculated by the process of automated re-weighing using the residual space which is discussed in
    depth in the (SEE THEORY TD). The implementation of this process can also be seen in the follwing diagram
    
    <figure><title>Flow Diagram of the Automated Re-Weighting Process of VCV Least Squares Anaysis</title>
        <graphic format="eps" fileref="gen-TD-ReweightFlow.eps">
    </figure>

    </para>

    <para>One important implementation problem that was discovered when applying this method was that in some cases the solution
    did not converge, and thus the program was remained in an infinite loop because the exit condition was never met. Another
    problem occured where it appeared that the re-weighting process continued until every point in the data set had been weighted 
    out. Both of these problems stemed from the fact that in our implementation of this re-weighting algorithm, 
    for over all design reasons, no point from data was ever entirely removed from the system but set it to 0 inside the 
    weighting matrix, which achieved the same effect in the data domain. In the residual domain, however, the weighted out points 
    were still being included due to the fact that the caulation of residuals does not require the use of a weighting matrix 
    (TD EQU 14).  The solution to this problem was to pre-multiply the residuals with the weighting matrix and removing 
    any residual equal to zero before calculating the thresholds for determining whether a data point was to be weighted out or not.
    </para>

    <para>The third method of Time Domain Analysis offered by the <command>GDMS</command> is the calculation of Random Walk Least
    Squares solutions. This method differers from the first two, in that a Random Walk weighting matrix is not a strictly diagonal
    matrix. At this time there is limited support for Random Walk Time Domain Analysis and although the <command>GDMS</command> 
    is capable of producing a solution, the user must specify the Random Walk weighting matrix to be used. This is largely due to
    the fact that at this time there is still no general consensus as to how a Random Walk matrix is to be produced.
    </para>

  </sect3> <!--Implemetation-->

  <sect3><title>Optimisation</title>
    <para>As one of the goals of <command>GDMS</command> speed of operations, great care has been taken to optimise the caculation
    speed of solutions in the Time Domain. This includes employing the use of passing values by reference to and from 
    functions where ever possible, thus saving the time that it takes to replicate the parameters being passed in memory. 
    The main optimisation, however, takes place in the area of matrix multiplication. It can be seen from (TD EQUATION 7 and 14) 
    that this is the most often used operation in calculating a Least Squares 
    solution and is by far the slowest of all the matrix operations. This is due to the fact that in a normal matrix 
    multiplication if, for example, we were multiplying matrix A by matrix B where A is <command>m</command> x 
    <command>n</command> and B is <command>n</command> x <command>o</command> it would take <command>m</command> x 
    <command>n</command> x <command>o</command> additions plus <command>m</command> x <command>n</command> x <command>o</command> 
    multiplications to reach as solution. Conversely, if certain
    properties are known about the matrices that are being used, the number of computations can be greatly reduced. For instance,
    in VCV Time Domain analysis we know that the weighting matrix P is always strictly diagonal, that is, every
    value of <command>P<subscript>ij</subscript></command> = 0 where i &ne; j. This means that when we calculate
    <command>A<superscript>T</superscript>P</command>, for instance, where <command>A</command> is <command>n</command> x 2 and
    <command>P</command> is <command>n</command> x <command>n</command> we need only calculate
    <command>A<subscript>ij</subscript>P<subscript>ii</subscript></command> for each element in the resulting matrix. Thus the
    problem has been reduced from being 2 x <command>n<superscript>2</superscript></command> additions and
    2 x <command>n<superscript>2</superscript></command> multiplications to just 2 x <command>n</command> multiplications.
    </para>
    
    
    <para>Similar calculation reduction can also be achieved by using the properties of the design, or <command>A</command> matrix.
    By using the property that the second row the the design matrix is always one, and the fact that anything multiplied by one is
    itself, we can further reduce the the number of calculations made in our previous example to just <command>n</command>
    multiplications. To put this into perspective, if we were carry out this matrix multiplication on an 
    <trademark class='registered'>Intel</trademark> <trademark class='registered'>Pentium 4</trademark> desktop processor
    at 2.0 GHz where each double precision multiplication operation takes 8 instruction cycles and each double precision 
    addition takes 6 instruction cycles(http://developer.intel.com/design/pentium4/manuals/248966.htm, 2002). 
    If we assume that matrix  <command>A</command> is a 1000 x 2 matrix and <command>P</command> is 1000 x 1000, it would take 2 x
    1000<superscript>2</superscript> multiplications 2 x 1000<superscript>2</superscript> additions and 14 ms to complete this
    operation. Conversly, by using the optimised method the number of operations required to compute this matrix equation reduces to
    1000 multiplications or 50 &mgr;s.     
    </para>
  </sect3><!--Optimization-->

  <sect3><title>Future Enhancements</title>
    <para>The <command>GDMS</command> offerers much functionality in the way of Time Domain Analysis, especially when VCV weighting
    matrices are employed. It does currently, however, only have limited support of Random Walk Time Domain Analysis. It is hoped
    that once an agreement has been reached as to how to specify a Random Walk Matrix it will be fully integrated into this
    product.
    </para>
  </sect3> <!--Future Enahncements-->

</sect2> <!--TDA-->


    <!--these sections will sit under sect1 - Implementation Issues-->

<sect2><title>Interpolation</title>

  <sect3><title>Introduction</title>
    <para></para>
  </sect3> <!--Introduction-->

  <sect3><title>Research</title>
    <para>
     - Splines etc..
     - Which algorithms?
     - Assess alternative algorithms
    </para>
  </sect3> <!--research-->

  <sect3><title>Implementation</title>
    <para>
       - How was it done?
       - Speed/ Optimisations
    </para>
  </sect3> <!--Implementation-->

  <sect3><title>Future Enhancements</title>
    <para>
    </para>
  </sect3> <!--Future Enahncements-->

</sect2> <!--Interp-->

    <!--these sections will sit under sect1 - Implementation Issues-->

<sect2><title>Windowing</title>

  <sect3><title>Introduction</title>
  <para></para>
  </sect3> <!--Introduction-->

  <sect3><title>Research</title>
    <para>
      Locating algorithms for the implementation of the initial windowing algorithms was trivial. The majority of these are well
      documented and are easily implemented. Dolph-Chebyshev window algorithm was a little more elusive. Initial attempts to implement
      this algorithm were unsuccessful. Two different tacks were taken: Initially an attempt was made to implement this purely in the
      time domain, based on an algorithm sourced from the speech recognition research at the Institute for Signal and Information Processing,
      ISIP, located at Mississippi State University. After many hours spent trying to validate the algorithm, it was decided that this could
      not be used. A second attempt was made using a window generated int the frequency domain, using code based on the original 1947 paper
      by Dolph. This algorithm only supported odd window sizes, and its use of the IFFT required that its size be 2<superscript>n</superscript>.
      This was unsatisfactory. The final algorithm that was implemented is based on FORTRAN code published in 1977 by
      IEEE Acoustics, Speech, and Signal Processing Society,
    </para>
  </sect3> <!--research-->

  <sect3><title>Implementation</title>
  <para>
    The windowing subsystem is designed to be extensible. Using OO techniques, a general algorithm for generating the window is
    combined with a specialised class which generates the individual coefficient. This means that to add another algorithm costs
    as little 10 lines of code, including user interface integration.   
    <equation>
      <title>Windowing Class Structure</title>
      <alt>Class Structure for Windowing Subsystem</alt>
      <graphic format="eps" fileref="gen-windowing.eps">
    </equation>    
  </para>
  <para>
    For all windows, coefficients can be generated for an arbitrary window size. The requirement that the windowed data be
    transformed to the Fourier domain imposes further limitation in that the Fourier libraries we are using will only operate on a
    data set of size 2<superscript>n</superscript>, 0 &lt; n &lt; inf. This restriction has been implemented at the UI level,
    maintaining the flexibility of the windowing subsystem.
  </para>
  <para>
       The actual algorithms implemented are as follows
    <itemizedlist>
        <listitem>
           <para><command>Hanning</command>: w(i) = &agr; - (1-&agr;).cos( 2.&pgr;.i/(size-1) ), for &agr;=0.5</para>
        </listitem>
        <listitem>
          <para><command>Hamming</command>:  w(i) = &agr; - (1-&agr;).cos( 2.&pgr;.i/(size-1) ), for &agr;=0.54</para>
        </listitem>
        <listitem>
          <para><command>Blackman</command>: w(i) = 0.42 - 0.5.cos( (2.&pgr;.val)/(size-1) ) + 0.08.cos( (4.&pgr;.i)/(size-1));</para>
        </listitem>
        <listitem>
          <para>
            <command>Dolph-Chebychev</command>: This is implemented as the inverse DFT of the Chebyshev
            polynomial evaluated at N equally spaced frequencies on the unit circle. The equation for the nth
            order Chebyshev equation calculated at x is
          </para>
          <para>
            cheb(n,x) = cos(n.acos(x))
          </para>
        </listitem>  
     </itemizedlist>
   </para>

   <sect4><title>Optimisation Issues</title>
   <para>
     Due to the simple nature of the majority of the algorithms in this module, speed is not generally an issue. With the exception
     of chebyshev, the most expensive calculation involves 2 cosine claculations and 4 multiplies. Since the size of the windows used
     tend to be small this is a relatively insignificant overhead. Dolph-Chebyshev is a little different. since this window is
     calculated in the Fourier domain it is computationally more expensive. Additionally, do support the flexibility of the windowing
     subsystem, this algorithm implementes its own IFFT. The Again, due to the small size of the window used, even
     this cost is minimal. The return on this cost is flexibility and confidence in the integrity of the algorithm.
   </para>
   <para>
     To support performance concerns and to speed the process as a whole, the number of calculations performed int his subsystem has
     been carefully considered and recalculation of the coefficients is only performed when it is the unavoidable.
    </para>
   </sect4>
     
  </sect3> <!--Implementation-->

  <sect3><title>Future Enhancements</title>
  <para>
    The set of algorithms supplied in this release is by no means exhaustive. There are some notable absences, such as the Kaiser
    window, which would be a valuable addition to the repertoir of <command>GDMS</command>. Due to the flexible design of this
    subsystem, addition of another algorithm can be achieved at minimal expense and is encourged.
  </para>
  <para>
    Another avenue which might be explored with regard to enhancements would be the simplification and optimisation of the
    Dolph-Chebyshev algorithm. This wasa a port of a FORTRAN function and was implemented faithfully with maintenance of
    data integrity a primary concern. This algorithm uses an inverse fourier transform internally and for windows of significant
    size this could prove to be expensive. Optimisation of this might prove to be benficial if window sizes are becoming large.
  </para>

  </sect3> <!--Future Enahncements-->

</sect2> <!--Windowing-->

    <!--these sections will sit under sect1 - Implementation Issues-->

<sect2><title>Frequency Domain Analysis</title>

  <sect3><title>Introduction</title>
    <para> 
    Being a common data analysis tool, there are a number of FFT libraries available in various languages. We therefore decided 
    that it may be more efficient to use a third party library for a number of reasons and given that there were many
    available, we assumed that at least one package could be found that would meet our requirements. Some considerations
    affecting our choice of FFT packages is the format of the FFT routine, the language they were written in and whether 
    it was a stand alone library, or simple class or a template class.
    </para>
  </sect3> <!--Introduction-->

  <sect3><title>Research</title>
    <para>
       The first library found in our research was the Fastest Fourier Transform in the West (FFTW) library from MIT. This 
       library is widely referenced and documented, and is reputed to be the most efficient FFT library in existence. The FFTW 
       library, however, was written in C, and was therefore discarded an attempt to maintain C++ consistency through the
       project. In addition, is the FFTW is quite large, and with the number of Dependant libraries for <command>GDMS</command> 
       increasing, it was determined that it was preferable to find a smaller library.
     </para>

     <para>
      Another candidate that was discovered after testing had already commenced with the FFTW library. This was a complex 
      template class to compute a FFT written in C++ (Arndt, 2002). This was considered as suitable choice as it was less complex 
      than FFTW, and also suited our loose object oriented design. A further benefit of using this implementation of a 
      FFT was due to the fact that it was a Template Class object was not restricted by type. This gave us greater flexibility in
      the type of data that could be FFT and what format the data would be returned in. For example, we might desire simple 
      <command>double</command>  values for PSD plotting, or perhaps <command>complex</command> values for further processing 
      after the FFT had been preformed. The FFT class also provides, the ability to perform inverse FFTs on a given set of data. 
      As far as the issue with speed was concerned, there was no evidence found during the testing of the complex template FFT 
      class to suggest that it would be much slower than FFTW. We therefore decided to integrate this implementation into the 
      project.
    </para>

  </sect3> <!--research-->

  <sect3><title>Implementation</title>
     <para>
       The FFT class could not be integrated without modifications. Firstly, there was an issue with the sign of the Fourier 
       transform. This refers to the FFT theory of this document where the standard Fourier formulae were discussed (SEE FD THEORY). 
       The original template class operates contrary to this standard and it appeared to calculate incorrect results. This problem 
       was overlooked in the initial algorithm selection process and appeared only later in the unit testing. Further information 
       specific to testing can be found in the Testing chapter later in this document. (TODO: YOU NEED TO EXPAND A BIT MORE HERE
       WASN'T IT BECAUSE THE FFT LIBRARY CALCULATED THE INVERSE OF WHAT YOU EXPECTED??)
    </para>

    <para>
      Another problem integration problem occurred because of the way in which the original template class deals with data 
      structures. Firstly, the class computes FFTs on and returns an array of data, where as <command>GDMS</command> uses matrices
      to pass data from class to class and thus it needed to be modified to accept this data type. Another change that was required
      was that the FFT library needed to modified to calculate a frequency scale in order to enable PSD plots to be graphed. 
      (TODO: EXPLAIN A BIT BETTER HOW THIS WAS DONE)
      TODO: yuk. The scale is
      calculated based on the dates provided in the input matrix. The period used for the calculation is one day.
    </para>
    <para>
       TODO: - Speed/ Optimisations. Maybe talk about how the original template class is written in terms of efficient code??
       (TODO:- YOU PROLLY DON'T NEED THIS AT ALL SEENING AS WE GOT THIS FROM SOMEWHERE ELSE)
    </para>
  </sect3> <!--Implementation-->

  <sect3><title>Future Enhancements</title>
   <para>
     In the frequency domain analysis, the <command>GDMS</command> package currently only carries out PSD plotting and inverse 
     IFFTs. Additional  functionality such as the FFT class returning the FFT data is a reasonably trivial task to complete (TODO: THIS 
     DOSEN'T MAKE ANY SENSE DO YOU MEAN COMPLEX DATA SO THAT IT CAN BE IFFT LATER?). 
     Its omission from the final specifications was solely due to time constraints. This functionality is already supported within
     the FFT framework, so adding this in future releases should be trivial. A further enhancement to the frequency domain 
     analysis of the <command>GDMS</command>, would be to allow the removal of erroneous data in the frequency domain, and then
     allowing the user to transform the data back into the time domain via an Inverse FFT. This would be an extremely useful function,
     as certain erroneous data, such as signal noise are not easily detected or removed in the time domain and this is much easier
     to carry out in the frequency domain. By allowing the transform of the remaining data back into the time domain, the effects of
     removing the erroneous signals can be determined.  
   </para>
  </sect3> <!--Future Enahncements-->

</sect2> <!--FDA-->

    <!--these sections will sit under sect1 - Implementation Issues-->

<sect2><title>CepMatrix and Template Classes</title>

  <sect3><title>Introduction</title>
  <para>At a very early stage in the design of the <command>GDMS</command> it became clear that some form of matrix object would be
  required, capable of fulfilling following requirements:-
  </para>
<itemizedlist>
    <listitem><para>Storage of a given data set and the results of any transformation preformed on it.</para></listitem>
    <listitem><para>Capability of preforming matrix operations including addition, subtraction,  with either another
    matrix or by a scalar value, division and transposition. In addition, support for many other operations was required
    in order to preform all the functionality of this system.</para></listitem>
    <listitem><para>Support for both two and three dimensional matrices.</para></listitem>
  </itemizedlist>
  
  <para>
  In addition, the matrix object itself had to be fast, flexible and reliable as it lies a the heart of the <command>GDMS</command>.
  In order to meet these requirements, therefore, the <command>cepMatrix</command> class template was created.
  </para>
  </sect3> <!--Introduction-->

  <sect3><title>Storage of data sets</title>
  
  <para>The first problem encountered in creating the <command>cepMatrix</command> was the need to support multiple
  data types. This was due to the fact that while the original data sets themselves as well as the transformations in the time domain
  were all of type <command>double</command>, once a transformation into the frequency domain was conducted the data became of type
  <command>complex</command>. This problem was overcome, however, by using one of the most powerful C++ functions, namely Template 
  Classes. A Template Class works by creating a class which is unrestricted by data type. The data type of the object itself
  is only specified when the object is instantiated, thereby allowing the same segment of code inside the Template Class to
  work with multiple types of data such as <command>int</command>, <command>float</command>, <command>double</command> and 
  <command>complex</command>. 
  </para>
  
  </sect3><!--Storage of data sets-->
  
  <sect3><title>cepMatrix operations</title>
  
  <para>Another key requirement of <command>cepMatrix</command> was the ability to carry out a whole range of matrix operations such
  as addition, subtraction, division and transposition. Much of this functionality was achieved by the use of  
  operator overloading. The method of operation overloading is another feature of C++ that allows the
  simplification of function calls, especially when relating to mathematical operations. For example, in order to add 
  matrices <command>A</command> and <command>B</command> together and save the results in <command>A</command> without operator
  overloading may look like this:- 
  </para>
  
  
  <programlisting>
  copy(A, multiply(A,B));
  </programlisting>
  
  <para>
  Conversely, if the += operator were overloaded the problem would simplify to
  </para>
  
 <programlisting>
  A += B;
 </programlisting>  
  
  <para>
  which is much easier to understand and actually removes a function call. In <command>cepMatrix</command> the following operations
  are overloaded:-
</para>
  
  <itemizedlist>
    <listitem><para>A += B :- adds matrix B to matrix A and stores the result in matrix A</para></listitem>
    <listitem><para>A -= B :- subtracts matrix B from matrix A and stores the result in matrix A</para></listitem>
    <listitem><para>A *= B :- multiplies matrix A by matrix B and stores the result in matrix A</para></listitem>
    <listitem><para>A *= c :- multiplies matrix A by the scalar value c and stores the result in matrix A</para></listitem>
    <listitem><para>A /= B :- divides matrix A by matrix B stores the result in matrix A</para></listitem>
    <listitem><para>A = B  :- copies matrix B to matrix A</para></listitem>
    <listitem><para>A == B :- returns true if matrix A is equal to matrix B</para></listitem>
    <listitem><para>A != B :- returns true is matrix A is not equal to matrix B</para></listitem>
  </itemizedlist>
  
  
  <para>
  The <command>cepMatrix</command> Class Template also supports other operations built to facilitate implemnetation inside the
  <command>GDMS</command> application. This includes an operation to query the matrix and determine if it is a strictly diagonal
  and operations to determine the maximum and minimum values of a given column in a matrix. There is also an operation
  to resize a matrix by adding a given number of rows to it. In addition, there are several get and set accesor methods provided 
  for acess individual elements of a given matrix.
  </para>
 
  </sect3><!--cepMatrix operations-->
  
  <sect3><title>Two Dimensional and Three Dimensional Matrices</title>
  
  <para>A third, key requirement of the <command>cepMatrix</command> Template Class was support for both two and three dimensional
  matrices. This is due to the fact that when data is windowed it requires a new matrix for each frame that is returned. It was
  decided that the prefered method of dealing with this problem was to keep all the data inside one <command>cepMatrix</command>
  object rather than having to use another container such as an array or vector. This, in turn, enabled the passing of data between
  objects in a uniform way and simplified the function calls required to access the given data. 
  </para>
  
  <para>
  Due to the fact that it was not strictly necessary to add any extra functionality and time constraints, three dimensional 
  matrices are implemented purely as storage devices. As such, there is limited support for these matricies 
  inside this object, that is, they can be created, accessed and copied but they can not be used in any other matrix operations 
  such as multiplication, addition, subtraction and division.
  </para>
    
  <para>Both two dimensional and three dimensional matrices are stored as arrays and are treated as separate member variables within 
  the <command>cepMatrix</command> Template Classes. This approch was taken for several reasons, firstly due to the fact that
  <command>cepMatrix</command> is a Template class all member variables had to be of a primitive type, that is, other templates or
  objects can not be instantiated inside these classes. This restriction on Template Classes is actually a compiler dependant and 
  is not a limitation of the language specification, per se. It does, however, arise when using <command>g++</command> which is 
  the most widely use compiler on the operating systems in which the <command>GDMS</command> is required to be used. 
  This limitation, was therefore, applied the to <command>cepMatrix</command> class and thus each matrix was stored, 
  internally as an array. In addition, the decision to treat two dimensional and three dimensional matrices as separate member 
  variables was made for reasons speed in preforming matrix operations. This is due to the fact that if all matrices were stored 
  using the same member variable, that is, the three dimensional member variable it would mean that an extra memory reference 
  would be incurred each time a matrix element is accessed. Consequently, some matrix operations would take almost twice as
  long to complete.   
  </para>
  </sect3>
  
  
  
  <sect3><title>Future Enhancements</title>
  <para>Whilst the <command>cepMatrix</command> Template Class includes all the functionality required to of it to be a powerful
  tool inside the <command>GDMS</command> system there are a few enhancements that could be made. As mentioned, three dimensional 
  matrices, unlike two dimensional matrices have very limited functionality inside this class. In future releases, it would be
  anticipated that three dimensional matrices have all the functionality associated with two dimensional matrices. Other future
  enhancements that could be made, would be to allow greater flexibility in re-sizing matrices, the implementation of
  iterators and the overloading of the [] operators to make the <command>cepMatrix</command> Template Class a full C++ Template
  container.</para>
  </sect3> <!--Future Enahncements-->

</sect2> <!--Template Classes-->

    <!--these sections will sit under sect1 - Implementation Issues-->

<sect2><title>User Interface</title>

<sect3><title>Introduction</title>
<para>
The <command>GDMS</command> user interface is the most prominent part of the application. It is the main way in which the user experiences the application. <command>GDMS</command> currently supports two user interface variants:
</para>

<itemizedlist>
<listitem><para>The X windows user interface (the program called <command>GDMS</command>)</para></listitem>
<listitem><para>The batch user interface (the program called <command>gdms-batch</command>)</para></listitem>
</itemizedlist>

<para>
This section will deal primarily with the research undertaken for implementing the X windows user interface. This is not only due to the fact that the scale of implementation required was much greater for this variant, however, also because there were many more alternatives available.
</para>
</sect3> <!--Introduction-->

<sect3><title>Graphical user interface</title>
<para>
There are a plethora graphical user interface toolkits available today. As each has it's advantages, a small survey of the options considered, and then a discussion of why <emphasis>wxWindows</emphasis> was selected is appropriate.
</para>

<sect4><title>CDE</title>
<para>
The Common Desktop Environment (CDE) was jointly developed by Hewlett-Packard, IBM, Novell and Sun Microsystems. These companies have subsequently adopted CDE as the standard window manager as shipped on a variety of their various operating systems (Chapman 2002). CDE uses the Motif windowing toolkit.
</para>

<figure><title>Sun Microsystem's default CDE configuration (Chapman 2002)</title>
<graphic format="eps" fileref="gen-sun-cde.eps">
</figure>

</sect4>

<sect4><title>CDE</title>
<para>
KDE development started in October 1996 (KDE 2002), as a replacement for Sun's CDE. KDE is based on Trolltech's qt widget set (Trolltech 2002). KDE is available as an installation option on the majority of the Linux distributions available.
</para>

<figure><title>The default KDE theme, Konquerer (KDE 2002)</title>
<graphic format="eps" fileref="gen-kde300-snapshot2-320x240.eps">
</figure>
</sect4>

<sect4><title>Gnome</title>
<para>
Gnome is the default window manager and windowing toolkit set for the majority of Linux distributions, including Linux, which the Survey Lab at the University of Canberra makes extensive use of. Gnome is implemented using the GTK windowing toolkit (GTK 2002).
</para>

<para>
Sun Microsystems have recently announced that from Solaris 8 on wards, Gnome will be available as a window manager. They have also announce that CDE will be replaced with Gnome in future releases (Sun Microsystems 2002).
</para>

<figure><title>The Solaris version of the GNOME desktop (Gnome 2002)</title>
<graphic format="eps" fileref="gen-gnome.eps">
</figure>
</sect4>

<sect4><title>Microsoft Windows</title>
<para>
Whilst not a X windows toolkit, Window's MFC toolkit was considered as a development alternative for <command>GDMS</command>. However, the survey lab does not currently contain any Microsoft Windows machines, so this option was not pursued any further.
</para>

<figure><title>The Windows XP desktop (Microsoft 2002)</title>
<graphic format="eps" fileref="gen-mfc-xp.eps">
</figure>
</sect4>

<sect4><title>wxWindows</title>
<para>
Whilst wxWindows is not a window manager in the same sense as all the other window managers discussed in this section, it has a variety of advantages relevant to <command>GDMS</command>. These include:
</para>

<itemizedlist>
<listitem><para><emphasis>The ability to use a variety of windowing toolkits</emphasis>. wxWindows supports GTK and Motif for Unix and Unix-like operating systems. Windows is also supported.</para></listitem>
<listitem><para><emphasis>Cross platform functionality</emphasis>. This allows <command>GDMS</command> to run on a variety of Unix and Unix-like operating systems.</para></listitem>
</itemizedlist>
</sect4>

</sect3>

<sect3><title>Windowing toolkit selection</title>
<para>
Based on the factors outlined above, the group selected wxWindows for the user interface implementation of <command>GDMS</command>.
As previously mentioned, all of these toolkits require developers to write in C++, which was one of the determining factors in the language choice for the development of the application.
</para>
</sect3>

<sect3><title>Batch user interface</title>
<para>
There are two common implementation methodologies for scripting languages. Both of these were considered for the <command>GDMS</command> implementation. The most common manner for implementing the parser and grammar for a scripting language is by using the compiler construction tools <command>yacc</command>, and <command>lex</command> (or their free versions, <command>bison</command> and <command>flex</command>). 
</para>

<sect4><title>Compiler construction tools</title>
<para>
<command>Lex</command> and <command>flex</command> are parser generation tools. They take a lexical specification file, and generate a LALR finite state machine for matching input strings (Aho, Sethi &amp; Ullman 1986). A sample of a lexical specification is shown below (in this case for the PDF file format):
</para>

<programlisting>
&lt;INITIAL&gt;\%PDF-[0-9]+\.[0-9]+    {
  debuglex(yytext, -1, "version");
  yylval.sval.data = (char *) returnStr(yytext, -1);
  yylval.sval.len = yyleng;
  return VERSION;
  }

&lt;INITIAL&gt;[+-]?[0-9]+\.[0-9]+     {
  debuglex(yytext, -1, "floating point");
  yylval.sval.data = (char *) returnStr(yytext, yyleng);
  yylval.sval.len = yyleng;
  return FP;
  }

&lt;INITIAL&gt;[+-]?[0-9]+\.[0-9]+\&gt;\&gt; {
  debuglex(yytext, -1, "floating point");
  yyless(yyleng - 2);
  yylval.sval.data = (char *) returnStr(yytext, yyleng);
  yylval.sval.len = yyleng;
  return FP;
  }
</programlisting>

<para>
In this example, two tokens are defined, VERSION, and FP. The syntax for these specifications is relatively simple. The first field, which is wrapped in angled brackets, is the name of the state that the specification should be used in. All of the examples shown here are for the default state, known as INITIAL (Levin, Mason &amp; Brown 1990). What follows this is a posix compliant regular expression to be matched. Finally, within the braces lies C code to execute when a match occurs.
In each of these examples, the C code writes a log entry for the match, sets up the data structure symbolizing the match, and then returns the name of the lexicial token which was matched.
</para>

<para>
<command>Yacc</command> and <command>bison</command> define the grammer which uses the tokens defined by <command>lex</command> or <command>flex</command>. In effect, this defines valid orders for the tokens to appear in, and what operations should be performed when a set of tokens is matched. Error handling for undefined token combinations is also supplied by the grammar. This is done through the use of hooks for reporting to the user in a manner which is appropriate to that particular application.
</para>

<para>
A sample grammar specification, in this case for SQL (Connolly &amp; Begg 1998) is as follows:
</para>

<programlisting>
SQL      : create SQL | insert SQL | select SQL
         |
         ;

create   : CREATE TABLE STRING '(' colvalspec ')' ';' {}
         ;

insert   : INSERT INTO STRING '(' colvalspec ')' VALUES 
           '(' colvalspec ')' ';' {}
         ;

select   : SELECT cvsaster FROM STRING {} wsel ';' {}
</programlisting>

<para>
A grammar consists of three building blocks. Each of the tokens which can be returned by the lexer is termed a <emphasis>terminal</emphasis>. Examples from above include <emphasis>CREATE</emphasis>, <emphasis>TABLE</emphasis>, <emphasis>STRING</emphasis>, <emphasis>INSERT</emphasis>, <emphasis>INTO</emphasis>, <emphasis>VALUES</emphasis>, and <emphasis>SELECT</emphasis>. These terminals are used by <emphasis>non-terminals</emphasis>, such as <emphasis>SQL</emphasis>, <emphasis>create</emphasis>, <emphasis>insert</emphasis>, and <emphasis>select</emphasis> from the example above. The rule which groups a non-terminal with a series of other non-terminals, and terminals, is called a <emphasis>production</emphasis> (Aho, Sethi &amp; Ullman 1986).
</para>

<para>
This grammar defines a language in which a SQL statement can consist of either a <emphasis>create</emphasis>, an <emphasis>insert</emphasis>, or a <emphasis>select</emphasis> statement, as shown by the SQL production. The empty non-terminal on the SQL production ensures that an end of file or empty line is also matched.
It should also be noted that the SQL production is recursive, except for this case which matches the empty line or end of file.
</para>

<para>
TODO C++ integration issues
</para>
</sect4>

<sect4><title>Hand coded parsers</title>
<para>
The alternative to using the compiler construction tools is to develop a language parser by hand. Depending on the complexity of the grammar to be implemented, this can be quite a realistic alternative. On the other hand, if the grammar is complex, then it can rapdily escalate into task of much larger scale, comparible to that of the rest of the application.
This option was found to be the most suitable for the <command>GDMS</command>. The grammar for the scripting language implemented by the batch version of <command>GDMS</command> is relatively simple,
in that all commands are parsed based on only one line, and no recursion within productions is required.
</para>
</sect4>
</sect3>

  <sect3><title>Implementation</title>
  <para>
       - How was it done?
       - Speed/ Optimisations
  </para>      
  </sect3> <!--Implementation-->
  
<sect3><title>Integration</title>
<para>
A significant portion of the <command>GDMS</command> user interface development was integrating the various classes and features with the graphical user interface.
  This integration has been facilitaded by a standard process for integrating mathematical functionality. The process is as follows:
</para>

<itemizedlist>
  <listitem>
    <para><emphasis>All functionality is either accessed from the mouse, or a menu</emphasis>.
       This leads to a consistent user interface, and minimises user confusion.
  </para></listitem>
  <listitem>
    <para>
      <emphasis>The output of all mathematical operations is a new dataset</emphasis>,
      and hence a new tab in the main application window. This has several advantages, including:-
        <itemizedlist>
          <listitem><para>Facilitating the implementation of the batch interface to the application.</para></listitem>
          <listitem><para>Allowing ease of comparison of the dataset before and after the application of the mathematical operation</para></listitem>
          <listitem><para>And TODO SOMETHING ELSE HERE.</para></listitem>
        </itemizedlist>
    </para>
  </listitem>
</itemizedlist>

<para>
Further documentation of the actual functionality available in the <command>GDMS</command> application is available in the user manual.
</para>
</sect3> <!--Integration-->


<sect3><title>Future Enhancements</title>
<para>
Whilst the user interface for <command>GDMS</command> is fully functional, there are a variety of improvements which could be made in the future. These include:
</para>

<itemizedlist>
<listitem><para><emphasis>Online help</emphasis>: the application does not currently provide any help in the user interface. wxWindows supports the display of HTML files within windows, which would an efficient and standard method for the implementation of online help within the application.</para></listitem>
<listitem><para>TODO</para></listitem>
<listitem><para>TODO</para></listitem>
<listitem><para>TODO</para></listitem>
<listitem><para>TODO</para></listitem>
</itemizedlist>
</sect3> <!--Future Enahncements-->


</sect2> <!--UI-->

  </sect1>

</chapter>

<chapter id="ch04"><title>Testing</title>
  <!--these sections will sit under sect1 - Implementation Issues-->

<sect1><title>Testing</title>


  <sect2><title>Introduction</title>
    <!-- TODO - reference this properly and fill in the blanks -->
    <para>From the outset it was decided that a testing regime was require to support debugging of subsytems and validation of
    mathematical routines. The basic requirements for the testing infrastructure were:
    <itemizedlist>
    <listitem><para><emphasis>Standardised:</emphasis> for all developers</para></listitem>
    <listitem><para><emphasis>Automated:</emphasis> to allow test execution with no user input</para></listitem>
    <listitem><para><emphasis>Simple:</emphasis> so as to minimise load placed on developers</para></listitem>
    <listitem><para><emphasis>GPL:</emphasis> The code must be released under the <command>GPL</command></para></listitem>
    </itemizedlist>
    </para>
  </sect2>

  <sect2><title>Implementation</title>
    <para>Unit testing, as adopted by the extreme programming community, was chosen as the ideal framework to support our system.
    A number of different libraries were examined as possible canidates and <command>CPPUnit</command> was selected as the most
    appropriate.
    </para>
    <para>
    Testing has been used extensively to support the validation of the mathematical subsystem. There are currently 84 tests spread across
    the 10 mathematical modules.
    </para>
  </sect2> <!--Introduction-->

  <sect2><title>User Interface Testing</title>
    <para>
    The user interface is exhaustively tested across all datasets. The tests are generated automatically and seek to locate datasets with
    peculiarities that might cause failure of the plotting library. There are currently 1131 tests dedicated to exercising this body of
    code.
    </para>
  </sect2> <!--Introduction-->


  <sect2><title>Conclusion</title>
    <para>
    The testing framework proved invaluable in the process of bug isolation and resolution and resolution. An additional benefit has been
    the early recognition of design problems, such as tight coupling between the subsystems and the user interface at an early stage in the
    project. Insummary, without the support of a system such as CPPUnit, the <command>GDMS</command> would not have the level of robustness
    that it does at this time and mathematical validation would have been quite difficult.
    </para>
  </sect2>

</sect1> <!--Testing-->

</chapter>

<chapter id="ch05"><title>Documentation</title>
<sect1><title>Introduction</title>

<para>
  Documentaion is of signifiant importance in any project, particularly with those that are not confined to a finite scope.
   The <command>GDMS</command> is one such project. In time series analysis system , there are an enormous number of 
    possible features. In a finite session such as this project, a careful balance must be drawn between the functionality 
    and the documentation. We needed to maximise our coding time whilst still providing sufficient documentation to both meet 
    the requirements and include all of the necessary information that might be needed by any future developers and users. 
    This led to our decision to use dools to assist with the documentation.
</para>

</sect1>

<sect1><title>Docbook</title>

<para>
 Docbook is system for writing documents using SGML or XML. It facilitates structure and is well suited for writing books or \
 papers of this kind. Unlike formal wordprocessors, docbook effectively takes the effort out standard formatting chores, such as;
 ensuring title fonts are consistent in type and size, indexing tables and equations and inserting table of contents. These features
 are provided free of effort by docbook. Furthermore, document layout, justification and other such features are dealt with 
 automatically by docbook. The syntax used for our document is SGML, being very similar to HTML, with which most of us 
 are already familiar reducing the learning curve which might otherwise render the process counter productive.
</para>

<sect2><title>Docbook Tools</title>
<para>
  Docbook has another benefit which was the selling point for its encorperation into the documentation process. This
  benefit is that it lends itself very well to automation. This achieved by using Docbook tools, a suite of programs
  used to generate the docbook output. The process is essentially the same as that compiling a source, in that a makefile is used
  to target the desired files. The files can then be structured in a hierarchical manner to ensure consistent and easy
  organisation. A typical document might contain a top level file, say thesis.sgml which links the other files, for example,
  timedomain.sgml and theory.sgml. This file also contains the top level information such as whether the document is a
  book or an article, in which case various pages are generated automatically by docbook accordingly. Thesis.sgml might
  also conatin a preface and glossary and other such general information sections.
</para>

<para>
  The target SGML files are linked from the top level file by issuing a command specifying the target file. The
  following example illustrates a top level file for a simple book:
</para>

<programlisting>
&lt;book&gt;
  &lt;bookinfo&gt;&lt;title&gt;Geodetic Date Modelling System: Thesis&lt;/title&gt;
     &lt;execute&gt;&lt;cmd&gt;builddb&lt;/cmd&gt;&lt;input&gt;authorgroup.sgml&lt;/input&gt;&lt;/execute&gt;
     &lt;execute&gt;&lt;cmd&gt;builddb&lt;/cmd&gt;&lt;input&gt;abstract.sgml&lt;/input&gt;&lt;/execute&gt;
  &lt;/bookinfo&gt;
  &lt;chapter id="ch01"&gt;&lt;title&gt; Chapter 1 - Introduction and Theory&lt;/title&gt;
    &lt;execute&gt;&lt;cmd&gt;builddb&lt;/cmd&gt;&lt;input&gt;Introduction.sgml&lt;/input&gt;&lt;/execute&gt;
    &lt;execute&gt;&lt;cmd&gt;builddb&lt;/cmd&gt;&lt;input&gt;Theory.sgml&lt;/input&gt;&lt;/execute&gt;
  &lt;/chapter&gt;
  &lt;chapter id="ch02"&gt;&lt;title&gt; Chapter 2 - Implemetation&lt;/title&gt;
    &lt;execute&gt;&lt;cmd&gt;builddb&lt;/cmd&gt;&lt;input&gt;Implementation.sgml&lt;/input&gt;&lt;/execute&gt;
    &lt;execute&gt;&lt;cmd&gt;builddb&lt;/cmd&gt;&lt;input&gt;Conclusion.sgml&lt;/input&gt;&lt;/execute&gt;
  &lt;/chapter&gt;
  &lt;chapter id="ch03"&gt;&lt;title&gt; Chapter 3 - Conclusion&lt;/title&gt;
    &lt;execute&gt;&lt;cmd&gt;builddb&lt;/cmd&gt;&lt;input&gt;Conclusion.sgml&lt;/input&gt;&lt;/execute&gt;
  &lt;/chapter&gt;
&lt;/book&gt;
</programlisting>

<para>
  The above example describes a simple book with three chapters. Each of the target SGML files would in turn be
  divided into section within the chapter. This method of documentation has its benefits. some of these being:-
</para>

<itemizedlist>
<listitem><para>Documentation changes need only be made in the one place, in this case a single file. 
                This is advantageous in case where text is repeated, much like the re-use concept in 
		programming.</para></listitem>
<listitem><para>Formatting and layout issues are dealt with by docbook.</para></listitem>
<listitem><para>The layout conforms to a general standard adheared to by a large majority of publishers.</para></listitem>
<listitem><para>Table of Contents and indexing of equations and figures are dealt with automatically by docbook.</para></listitem>
</itemizedlist>

<para>
  Our use of docbook, however, was not altogether seemless. We found version inconsistencies in different
  Linux distributions. Once we updated all of the development machines to use the same version of docbook, we ran into
  further trouble. It seemed still that the different distributions were not consistent when it came to building the
  documentation. An example of this, is that on some of the machines, although the docbook scripts would build the
  document, it was incomplete. This was due to the fact that SGML syntax errors were not being caught by the scripts
  and as a result, later in the build process when the build finally broke, it was extremely difficult to pin point
  exactly where the problem was. The same docbook version used on a different distribution caught the SGML errors allowing
  us to debug and fix any problems. Furthermore, the templates on the different distributions varied such that page numbering
  was not always consistent. This was infact the case even on different versions of the same distribution. To overcome
  these issues, we simply generated the final submission of the documentation on the most suitable template.
</para>

</sect2>

<sect2><title>Autodocbook</title>
<para>
 Autodocbook is tool employed for the documentation of source code. It is a simple perl script designed to parse
  C or C++ source files extracting comment blocks within specifi tag and turning them into docbook sgml files.
  These can then in turn be used to create  man pages, info pages and HTML documentation. This again serves to
  minimise the need to changes things in various places. Firstly for example, if comments need to be changed for a specific
  class or file, it is done and the Autodocbook script is run again requiring no further action. The second obvious
  situtation is in the creation of the other possible outputs; man pages, info pages and HTML files. The appropriate
  conversion script is run on the target files and the task is complete.
</para>

</sect2>
</sect1></chapter>


<chapter id="ch06"><title>Conclusion</title>
  <sect1><title>Conclusion</title>

<sect2><title>What did we do? - heading left for proofing only</title>
<para>
  The GDMS packages offers a suite of time series analysis tools intended for the processing of data from various sources.
  The package was designed and implemented to be as flexible and as platform independent as possible given the time constraints
  under which the group working. We aimed to provide a package which would enhance on other currently available analysis
  tools. This involved providing time domain analysis, frequency domain analysis and of course along with those, windowing and interpolation functionality.
  The loose object oriented design provides GDMS with a good deal  in the way of extensibility. The extensive documentation
  will provide both users and developers in the future with an understanding of exactly how the system was constructed facilitating
  changes that may be desired.
</para>
</sect2>

<sect2><title>Did we succeed? - heading left for proofing only</title>
<para>
  The measure of the success of a project is contributed to by a number of factors. Perhaps the first and foremost concern
  in a software application is whether or not it actually works. This is of course the most obvious measure of success to a user using the application.
  Having made this point, another one of significance is whether or not the application achieves was it was intended to achieve.
  In the case of GDMS, we started with a broad requirements description. Based on these requirements, a project proposal was written. TODO: attach?
  This proposal outlined our intentions for the GDMS package.
</para>

<para>
  Throughout the course of the project some requirements were deemed unnecessary, however, overall, the
  requirements of the system grew compared to what we expected, in terms of fundamental requirements of the system.
  In many case, this were functionality deemed essential in order to provide a system that was considered <emphasis>useful</emphasis>.
  In some case, these features were the finer detail of the broader requirements which we did not consider.
  As the deadline drew near, we needed to assess in order of priority, which functionality was essential and which wasn't.
  This was largely a role that Associate Professor Peter Morgan played, he being the key stakeholder in GDMS.
</para>

<para>
  Having finally implemented a working system, with functionality such that the system was considered <emphasis>useful</emphasis>,
  we succeeded. A critical point here, is that this system from the very beginning, was always to be an evolving
  one. Future project teams are expected to further enhance the functionality we have provided. Therefore, in terms of
  enhancing features of already available packages, we believe again, that we have succeeded. We have set up a
  platform upon which others can build, whilst providing something now. This is not to say that
  this is the definitive application for time series analysis, as that is clearly not the case. In fact, we do not
  consider this in itself, to be achievable in the time with which we had to complete this application. Time series analysis
  is a growing field of its own, and it would not be wise to expect to fill this area in a short finite session.
  We have however, succeeded in providing a package which enhances on some functionality of other applications, and
  can be further enhanced in the future.
</para>
</sect2>

<sect2><title>Good or bad result? - heading left for proofing only </title>
<para>
Kind of covered above
</para>

</sect2>

<sect2><title>What did we learn? - heading left for proofing only </title>
<para>
  A number of valuable lessons have been learned from the GDMS development process, not all of which relate directly
  to application development. That which we consider to be the most critical lesson here, is planning. As previously mentioned,
   the original requirements were broad and did not cover many of the finer details required. It was during our initial requirements
   definition process, that we should have sought to discover as much as possible about exactly what was involved
   with various features in the broad specification. Our mathematical body of knowledge before the project began, was
   considered at least enough to proceed, yet perhaps not quite enough to foresee these finer specification details.
   A more comprehensive requirements definition phases in this project would quite likely have reduced the number of
   problems we had later, with missing or in correct functionality. Having said this, the development of GDMS can be likened
   to a prototyping methodology. It was necessary to consult continuously with Associate Professor Peter Morgan, to ensure
   that project was meeting his requirements.
</para>
<para>
   This consultation was necessary and relevant for all aspects of the system. It is difficult to assign more importance
   to the mathematical functionality or the user interfce, as they are both critical here. Of course, the mathematical
   functionality must be correct for the system to be correct. The user interface however, is the vehicle with which we deliver
   feedback and output to the user. The manner in which the UI displays this ouput is critical here. It is important that the user
   of a system be able to relate to it. It would be counter productive for us to develop an application which varied
   signifantly from a standard form. Users of applications such as this one, will be largely made up of people who have been
   using similar tools for a considerable time. An application with a very different look and feel, is not likely
   to be adopted as a useful tool.
</para>

<para>
   Having now completed the first pass of this application's development, we can all say with confidence that our
   knowledge in a number of areas has increased significantly. The first is of course the aforemntioned planning.
   Even though all of us have been involved with software development of various kinds in the past, this has highlighted
   the critical importance of the requirements definition process. Having been programming in a Linux envrionment
   for the duration of the project, this too has been an error of exapnsion for our knowledge. This extends of course
   to the programming language itself, C++, various libraries such as that used for the user interface, and many more
   aspects of development. Furthermore, our knowledge of data analysis and processing has benifited here. Not only
   in terms of the theory and processes involved, but also the background, applications and vast significance of the analysis and
   processing of data in order to obtain information.
</para>

</sect2>

<sect2><title>Summarise Lessons - heading left for proofing only </title>
<para>Covered above.</para>
<para>Humpty dumpty sat on a wall, </para>
<para>Humpty dumpty had a great fall, </para>
<para>So the lesson is, that dumb old hum,pty dumpty shouldn't have been sitting there in the first place. </para>
</sect2>


<sect2><title>Summarise future enhancements - heading left for proofing only </title>
<para></para>
</sect2>

</sect1> <!--Conclusion-->


</chapter>

<!--TODO:- ENSURE GLOSSARY IS IN ALPHABETICAL ORDER!-->
<glossary><title>Glossary of Terms</title>

    <glossentry><glossterm>CVIEW</glossterm>
      <glossdef><para>
	   C File View 
        </para></glossdef>
    </glossentry>

    <glossentry><glossterm>DFT</glossterm>
      <glossdef><para>
      Discrete Fourier Transform
        </para></glossdef>
    </glossentry>

    <glossentry><glossterm>GAMIT</glossterm>
      <glossdef><para>
	   Global Positioning System At Massachusetts Institute of Technology 
        </para></glossdef>
    </glossentry>

    <glossentry><glossterm>GDMS</glossterm>
      <glossdef><para>
	   Geodetic Data Modelling System
        </para></glossdef>
    </glossentry>

    <glossentry><glossterm>GPS</glossterm>
      <glossdef><para>
	   Global Positioning System
        </para></glossdef>
    </glossentry>

    <glossentry><glossterm>FFT</glossterm>
      <glossdef><para>
      Fast Fourier Transform
        </para></glossdef>
    </glossentry>

   <glossentry><glossterm>FFTW</glossterm>
      <glossdef><para>
      Fastest Fourier Transform in the West, a C library to preform Fast Fourier Transforms.
        </para></glossdef>
    </glossentry>

    <glossentry><glossterm>ISIP</glossterm>
      <glossdef><para>
	   Institute for Signal and Information Processing, Mississippi State University
        </para></glossdef>
    </glossentry>

    <glossentry><glossterm>MIT</glossterm>
      <glossdef><para>
           Massachusetts Institute of Technology
        </para></glossdef>
    </glossentry>
    
    <glossentry><glossterm>NTB</glossterm>
      <glossdef><para>
           Normalised Transition Bandwidth. Used for tuning the Dolph-Chebyshev window
        </para></glossdef>
    </glossentry>

    <glossentry><glossterm>PSD</glossterm>
      <glossdef><para>
      Power Spectral Density
        </para></glossdef>
    </glossentry>

    <glossentry><glossterm>SLR</glossterm>
      <glossdef><para>
 	    Satellite Laser Ranging
        </para></glossdef>
    </glossentry>
    
    <glossentry><glossterm>SQL</glossterm>
      <glossdef><para>
 	    Structured Query Language
        </para></glossdef>
    </glossentry>

    <glossentry><glossterm>TSVIEW</glossterm>
      <glossdef><para>
           Time Series View
        </para></glossdef>
    </glossentry>
    
    <glossentry><glossterm>VCV</glossterm>
      <glossdef><para>
           Variance Co-variance
        </para></glossdef>
    </glossentry>
    
    <glossentry><glossterm>VLBI</glossterm>
      <glossdef><para>
           Very Long Base Line interferometry
        </para></glossdef>
    </glossentry>


</glossary>

<!--End Glossary-->

<chapter><title>References</title>
  <!--TODO: Make sure there are no repeated entries-->

<!--Mikals references -->

<sect1><title>References</title>

<para>
Aho, A. V, Sethi, R., Ullman, J.D. 1986, <emphasis>Compilers: Principles, Techniques, and Tools</emphasis>, 
Addison-Wesley Publishing, Massachusetts.
</para>

<para>
Arndt, J. 2002, <emphasis>cplxfft.h, Complex Fast Fourier Transform Template Class (Part of FXT)</emphasis> [Online] 
Available: http://www.jjj.de/fxt/
TODO: WHEN WAS THE WEBSITE COPYRIGHTED OR LAST UPDATED. WHEN WAS IT ACCESSED?
</para>

<para>
Bellanger, M., 1984, <emphasis>Digital Processing of Signals</emphasis>, John Wiley and Sons, 
New York.
</para>

<para>
Carezia, A., 18 June 2002 [last update],
<emphasis>Dolph-Chebyshev Window</emphasis>,
[Online], Octave Sources, Available: http://www.octave.org/octave-lists/archive/octave-sources.2002/msg00019.html,
Jul-2002.
</para>

<para>
Chapman, M., 2002, <emphasis>Window managers for X</emphasis> [Online] Available: http://www.plig.org/xwinman/cde.html
TODO: WHEN WAS THE WEBSITE COPYRIGHTED OR LAST UPDATED. WHEN WAS IT ACCESSED?
</para>

<para>
Connolly, T., Begg, C. 1998, <emphasis>Database Systems: A practical approace to design, implementation, and management</emphasis>, Addison-Wesley Publishing, Massachusetts.
</para>

<para>
Cppunit Project, 11 April 2002 [last update],
<emphasis>CPP Unit, Unit Testing Library</emphasis>,
[Online], Sourceforge.net, Available: http://cppunit.sourceforge.net,
May-2002.
</para>

<para>Deitel, H.M., Deitel, P.J., 1998, <emphasis>C++: How to Program</emphasis>, Prentice-Hall, New Jersey.</para>
 
<para>
Flannery, B., Press, W., Teulosky, S., Vettering, W., 1988,
<emphasis>Numerical Recipes in C</emphasis>,
Cambridge University Press, Cambridge.
</para>

<para>
Fowler, M., 1999,
<emphasis>UML Distilled: A Brief Guide to the Standard Object Modeling Language</emphasis>,
Addison-Wesley, New York.
</para>

<para>
Gamma, E., Helm, R., Johnson, R., Vlissides, J., 1994,
<emphasis>Design Patterns: Elements of Reusable Object-Oriented Software</emphasis>,
Addison-Wesley, New York.
</para>

<para>
Geoscience Australia, unknown [last update],
<emphasis>National Mapping - Geodesy, Very Long Baseline Interferometry</emphasis>,
[Online], Geoscience Australia, Available: http://www.auslig.gov.au/geodesy/sgc/vlbi/vlbitech.htm,
15 November 2002.
</para>

<para>
Geoscience Australia, unknown [last update],
<emphasis>Satellite Laser Ranging</emphasis>,
[Online], Geoscience Australia, Available: http://www.auslig.gov.au/geodesy/slr/whatis.htm,
17 November 2002.
</para>

<para>
Gnome 2002 <emphasis>Gnome: Computing made easy</emphasis> [Online] Available: http://www.gnome.org
TODO: WHEN WAS THE WEBSITE COPYRIGHTED OR LAST UPDATED. WHEN WAS IT ACCESSED?

</para>

<para>
GTK 2002 <emphasis>GTK+ FAQ</emphasis> [Online] Available: http://www.gtk.org/faq/
TODO: WHEN WAS THE WEBSITE COPYRIGHTED OR LAST UPDATED. WHEN WAS IT ACCESSED?
</para>

<para>
Holzner S., 2001, <emphasis>C++ Black Book</emphasis>, The Coriolis Group, Scottsdale. 
</para>

<para>
Institute for Information and Signal Processing, 30 March 2002 [last update],
<emphasis>ISIP Software Documenation</emphasis>,
[Online], Mississippi State University,
Available: http://www.isip.msstate.edu/projects/speech/software/documentation/class/algo/Window/,
Jun 2002.
</para>

<para>
Intel.com, 2002 [copyright], <emphasis><trademark class='registered'>Intel</trademark>
<trademark class='registered'>Pentium 4</trademark> Optimization Reference Manual</emphasis> [Online], Intel Corperation, Avalible:
http://developer.intel.com/design/pentium4/manuals/248966.htm, 18 September 2002.
</para>

<para>
Interstellar Research, 1 August 2001 [last update],
<emphasis>Daqarta: FFT Windowing</emphasis>,
[Online], Daqarta.com, Available: http://www.daqarta.com/ww00wndo.htm,
Nov-2002.
</para>

<para>
Jeffereis, R.E., 14 January 2002 [last update],
<emphasis>XProgramming.com, an extreme programming resource</emphasis>,
[Online] Jeffereis, R.E., Available: http://www.xprogramming.com,
May-2002.

</para>

<para>
Junit Project, 1 August 2002 [last update],
<emphasis>JUnit, Testing Resources for Extreme Programming</emphasis>,
[Online], objectmentor.com, Available: http://www.junit.org/,
May-2002.
</para>

<para>
KDE 2002, <emphasis>KDE Home Page</emphasis> [Online] Available: http://www.kde.org
TODO: WHEN WAS THE WEBSITE COPYRIGHTED OR LAST UPDATED. WHEN WAS IT ACCESSED?
</para>

<para>
Lay, D.C., 1997, <emphasis>Linear Algerbra and its Applications (2nd edition)</emphasis>, Addison Wesley Longman Inc., Massachusetts
</para>

<para>
Lepple, C., 23 July 2000 [last Update], 
<emphasis>Fast Fourier Transforms Demystified</emphasis> 
[Online] Available: http://www.foo.tho.org/charles/fft.html,
November-2002.
</para>
  
<para>
Levine, J.R., Mason, T., Brown, D., 1990, <emphasis>lex &amp; yacc</emphasis>, O'Reilly, Farnham.
</para>

<para>
Lynch, P., 27 June 1996,
<emphasis>The Dolph-Chebyshev Window: A Simple Optimal Filter</emphasis>,
[Online] Availale: http://www.maths.tcd.ie/~plynch/Publications/Dolph.pdf,
Jul-2002.

</para>

<para>
Mathews, J.H., Fink, K.D., 1999, <emphasis>Numerical Methods Using MATLAB (3rd edition)</emphasis>, Prentice Hall, New Jersey.
</para>

<para>
Microsoft 2002 <emphasis>Windows XP screenshot</emphasis> [Online] Available: http://www.microsoft.com/presspass/newsroom/winxp/images/img016.jpg
TODO: WHEN WAS THE WEBSITE COPYRIGHTED OR LAST UPDATED. WHEN WAS IT ACCESSED?
</para>

<para>
Rabiner L.R, Schafer R.W., 1978, <emphasis>Digital Processing of Speech Signals</emphasis>, Prentice Hall.
TODO: WHERE WAS THIS BOOK PUBLISHED??
</para>

<para>
Royster, D., 15 Feburary 2000 [last updated], <emphasis>Box Plots and Box & Whiskers Plots</emphasis> [Online], University of
North Carolina at Charlotte, Avalible: http://www.math.uncc.edu/~droyster/courses/spring00/maed3103/Box_Plots.htm, July 2002.
</para>

<para>
Scott, B., Taylor, R., 29 July 1999 [last update],
<emphasis>What is Radio Astronomy</emphasis>,
[Online], University of Calgary, Available: http://www.ras.ucalgary.ca/Spacevlbi1.html,
17 November 2002.
</para>

<para>
Stroustrup, B., 2000,
<emphasis>The C++ Programming Language, Special Ed</emphasis>,
Addison-Wesley, New York.
</para>

<para>
Sun Microsystems 2002, <emphasis>GNOME 2.0 Desktop for the
<trademark>Solaris</trademark>
 Operating Environment </emphasis> [Online] Available: http://wwws.sun.com/software/star/gnome/
TODO: WHEN WAS THE WEBSITE COPYRIGHTED OR LAST UPDATED. WHEN WAS IT ACCESSED?

</para>

<para>
Trolltech 2002, <emphasis>Trolltech Home Page</emphasis> [Online] Available: http://www.trolltech.com
TODO: WHEN WAS THE WEBSITE COPYRIGHTED OR LAST UPDATED. WHEN WAS IT ACCESSED?

</para>

<para>
Unit++ Project, 4 May 2002 [last update],
<emphasis>The Unit++ Testing Framework</emphasis>,
[Online], Sourceforge.net, Available: http://unitpp.sourceforge.net,
May 2002.
</para>

<para>
Unknown, 5 November 2002 [last update],
<emphasis>Goddard Geodetic VLBI Group</emphasis>,
[Online], Space Geodesy Program, NASA, Available: http://lupus.gsfc.nasa.gov/vlbi.html,
17 November 2002.
</para>

<para>
Wittke, J., 29 October 2001 [last updated], <emphasis>Electron Microprobe Notes - Statistics</emphasis> [Online],
Northern Arizona University, Avalible: http://jan.ucc.nau.edu/~wittke/Microprobe/Statistics.html, 12 November 2002.
</para>

</sect1>


</chapter>  

</book>
