<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook V4.1//EN">
<book><bookinfo>
<title>GDMS: Thesis</title>
<authorgroup>

<author>
  <firstname>Daniel</firstname>
  <surname>Fernandez</surname>
  <affiliation><jobtitle>991672</jobtitle></affiliation>
</author>

<author>
  <firstname>Michael</firstname>
  <surname>Still</surname>
  <affiliation><jobtitle>964076</jobtitle></affiliation>
</author>

<author>
  <firstname>Blake</firstname>
  <surname>Swadling</surname>
  <affiliation><jobtitle>982087</jobtitle></affiliation>
</author>

<author>
  <firstname>Kristy</firstname>
  <surname>Van Der Vlist</surname>
  <affiliation><jobtitle>983118</jobtitle></affiliation>
</author>

<author>
  <firstname>Nick</firstname>
  <surname>Wheatstone</surname>
  <affiliation><jobtitle>983131</jobtitle></affiliation>
</author>

</authorgroup>
</bookinfo>



 <!--TODO:- ENSURE GLOSSARY IS IN ALPHABETICAL ORDER!-->
<glossary><title>Glossary of Terms</title>


    <glossentry><glossterm>GPS</glossterm>
      <glossdef><para>
	   Global Positioning System
        </para></glossdef>
    </glossentry>

    <glossentry><glossterm>SLR</glossterm>
      <glossdef><para>
 	    Satellite Laser Ranging
        </para></glossdef>
    </glossentry>

    <glossentry><glossterm>VCV</glossterm>
      <glossdef><para>
           Variance Co-variance
        </para></glossdef>
    </glossentry>
    
    <glossentry><glossterm>VLBI</glossterm>
      <glossdef><para>
           Very Long Base Line interferometry
        </para></glossdef>
    </glossentry>


</glossary>

<!--End Glossary-->

<chapter><title> Theory </title>
  <sect1><title>Background</title>

<para>
  This package is intended for use in the analysis of GPS <footnote><para>Global Positioning System</para></footnote>, VLBI
  <footnote><para>Very Long Baseline Interferometry</para></footnote> and SLR <footnote><para>Satellite Laser Ranging</para></footnote> data on
  tectonic movement. The data is extensive, covering around ten years of measurements from stations around the globe. The
  GPS, VLBI, and SLR data have various anomalies due to their different vulnerabilies to conditions, such as delay due to the
  ionosphere, temperature, and other considerations including projected stellite orbits.
</para>

<para>
  Various organisations around the world collect data from the aformentioned sources. This data provides information
  regarding among other things, tectonic plate movement. This allows organisations to draw conclusions to the level
  of continental drift, ocean levels, etc.
</para>

<para>
  Before the data can be interpreted, it must be processed. This, as with most raw data, involves shaping it into a format which can be
  readily understood by people, and in almost all cases software. Information from data is general more readily extracted from graphical
  forms, enabling for example, trends in the data to be seen if they exist. The next step is to be able to take into consideration various
  circumstances that may arrise during monitoring. There may be changes in equipment, malfunctionions, bad or whether, or as many cases,
  noise in the data. Before drawing any conlusions from the data, people need to be certain that they are dealing with only the necessary
  information.
  In order to extract the relavent information from the data, we need to perform various transformations on the data.
  Some of the techniques employed during this data processing include, Least Squares regression, Fast Fourier
  Transforms (FFT), Gabor transforms (specgrams) and Power Spectral Density plots (PSD).
</para>

<para>
  It is at this point, that software becomes an important tool. It can be used to model certain situtations or remove unwanted data. For
  example if a know malfunction was noted at a given time, the modelling process can deal with this event by perhaps removing the data,
  or applying we-weighting algorithms to the data etc.
</para>

<para>
  In many cases with data analysis, there is a need to view different representions of the data, for example, in the Frequency Domain.
  Once again, this may allow the identification of trend or patterns. What happens when the frequency of the monitoring decreases or
  increases? Such questions can assist in more accurate and informed conlcusions as to what exactly the data is telling us.
</para>

</sect1> <!--backgorund-->


  <sect1><title>Time Domain Analysis</title> 

<sect2><title>Introduction</title>
  <para> Analyisis of discrete signals in the time domain is essentual to the understanding of geodetic data such as that produced
  by GPS, VLBI and SLR. By applying a linear least squares regression model to the data underlying data trends can be asertained,
  such as, the average rate of contenintal drift. Linear regression analysis also allows such phenomenom as "Random 
  Walk" and white noise, that are knowen to affect each type of geodetic data to differing degrees to be asertained. This, in turn, allows
  enables the geodetic community to gain a greater understanding of which elements my have a greater effect on the accuracy of a
  given geodetic data set. In addition, an analysis of residuals resulting from a linear regression can aid in the dection and
  deletion of erronious data.        
  </para>
</sect2> <!--Introduction-->


<sect2><title>Theory</title>
  <sect3><title>Linear Least Squares Regression Modeling</title>
    <para>One of the major problems in discrete signal analysis that these systems are almost always inconsistant, that is, there is
    no exact solution to the equation of the line 
    
    EQUATION 1
    
    where <command>m</command> is the median and <command>c<command> is the y-intercept. This is usually due to the fact that there 
    are more data points in the system than those required to solve this equation. Linear least squares regression modeling is a 
    method use to calculate <command>m</command> and <command>c<command> such that it is the closest aproximation
    <command>y</command> for the given set of observations in the system, also known as a line of best fit. In the analyis of
    geodetic data the line of best fit is given as the solution to the matrix equation
    
    EQUATION 2
    
    where <command>X<command> is the corrections to the apriori estimates
    
    
    EQUATION 3
    
    and <command>n</command> is the number of observations in the system. The <command>A</command> matrix is the know as the design
    matrix
    
    EQUATION 4
    
    <command>P<command> is the <command>n</command> x <command>n</command> weighting matrix and <command>L</command> is the apriori 
    matrix 
    
    EQUATION 5 
    
    </para>
    
    <para>When carrying out least squares regression analyisis on geodetic data it is common to set the inital apriori estimates of
    the mean and y-intercept to zero thereby setting all value of <command>L<subscript>c</subscript></command> to zero. By setting
    this inital constraint the apriori matrix becomes
    
    EQUATION 6
    
    and hence Equation 2 becomes
    
    EQUATION 7
    </para>
        
    </para>Another common constaint use in this type of analyisis is to set y-intercept <command>c</command> at the point of the
    first time observation in the given system. This is done because the time format used in geodetic
    data systems is a decimal number where the numerator corresponds to the year and number of days into the year is denominator.
    For example the date value 2002.0014 corresponds to the Gregorian calander date of 12:00, 1 January 2002. If 
    x-axis was not constrained the y intercept would always be given at <command>x</command> = 0, which corresponds to a date of 
    4000 BC (about 500 years before the Bronze age). This would have the effect of rendering the any value caulated as a 
    y-intercept virually meaningless. Therefore, the x-axis is constrained by the equation            
    
    EQUATION 8
    
    where <command>t<subscript>0</subscript></command> is the first observation time in the given system. This has the effect of
    making the <command>c</command> value more meaningful and simplifying any graph of this data. 
    </para>
  </sect3><!--Linear Least Squares Regression Modeling-->
  
  <sect3><title>Weighting Matricies - Variance Co-variance and Random Walk</title>
    <para>It is not uncommon in geodetic data, especially those taken over a long period of time (ie a number of years) that some
    of the data value are less reliable than others. This can be due such factors as failure or replacement of data gathering 
    equipment(FIGURE STROMLO) or natural phenomena such as earthquakes(FIGURE COCO). It is important, therefore, to have some 
    mechanisum of ensuring that this, potentially erronous data, does not influence the linear regression analyisis in an adverse 
    manner. In order to do this what is known as a Variance Co-variance (VCV) weighting matrix is employed. 
    </para>
    
    <para>A VCV weighting matrix is a strictly diagonal matrix, that is 
    
    EQUATION 9
    
    which models a standard Gaussian distribution (also known as a bell curve) (FIGURE GAUSSIAN DISTRIBUTION). This method works by
    pre-multiplying the observation values with a number that represents its "correctness" (see equation 1.2). Therefore the 
    weighting value specified on the diagonal of the VCV matrix should be a percentage value ranging from one to zero, whereby a 
    value of one one would mean that that given data point fully participates in the calculation and a value of zero indicates that
    the data point was erronous and is disgarded. 
    </para>
    
    <para>Due to the nature of method of weighting data values, great care must be taken so that the linear regression model
    calculated is not distorted. This is due to the fact that, by pre-multiplying the observation values with a weighting matrix the
    observation values are essentually being changed. If too few erronous data points are un-weighted, the least squares line that
    is caculated will not accuratly represent the general data trend, simarly, weighting out too may data points will cause the
    same error. For example, a standard model for weighting an inconsistant system would be to assign a weighting value according to
    standard deviation, that is, for all points within standard deviation above or below the mean would be given a weight of 1.
    Similaryly, points within two standard deviations, but greater than one standard deviation would be weighted 0.5 and any point 
    two standard deviations above or below the mean would be given a weight of 0. This model, however, has been knowen to unaturally
    distort the data when applied to geodetic systems (REFERNCE, EXAMPLES!!!!!!!!!!). A far safer method of assigning a weighting
    matrix is to simply weight any data point greater than two standard deviations of the mean as zero and all other values as one
    (WHY! EXAPLES! REFRENCES!!!!!!!!!!!)
    </para>
    
    <para>Another use of weighting matricies is to emphasize any underlying data trends that may be affecting the system. One such
    trend is known as Random Walk, which is a phenomenon that the geodetic community believes occours, particularly in GPS data.
    Detection of this trend become more urgent in recent times becase it is believed that Random Walk can only be accruatly seen in
    systems that span at least six years, or more optimally 10 years and data sets of this lenght have not become avaliable until
    now. If it is assumed  
    
    EQUATION 10
    
    that is, the observation values are evenly distributed then the expected Random Walk values are 
    
    EQUATION 11
    
    and thus the weighting matrix is
    
    EQUATION 12
    
    It is anticipated that using such a weighting matrix should allow any underlying random walk trend to be seen. At this time,
    however, this is some debate as to which equation should be use in order to determine the random walk co-effients and thus the
    optimal weighting matrix. As a result, there is only limited support for this type of caculation within the scope of the
    project.
    </para> 
  </sect3><!--Weighting Matracies - Variance Co-variance and Random Walk-->
  
  <sect3><title>Residuals and Detetermining VCV Weighting Models</title></sect3>
    <para>Resisdual space is another important aspect of Time Domain analysis. In essence, a residual is vector that represents the
    distance between the observered value and the least squares regression line of best fit. The residual vectors are therefore
    caulated by the matrix equation
    
    EQUATION 13
    
    Where <command>L</command> is specified by Equation 1.5. If the same constraint is applied to the apriori matrix as was used in
    the linear least squares model, that is, it is assumed that or inital model has a gradient and y-intercept of 0, the residual
    equation becomes
    
    EQUATION 14
    
    </para>
    
    <para>At the very least, the residual matrix shows how well the regression line aproximates the given system, that is the 
    smaller the values of V the better the fit. In addition, if the residuals are transformed into the Frequency Domain via a Fast
    Fourier Transform, an optimal model would give a totaly flat frequency response. Another property of residuals is that
    
    EQUATION 15
    
    this is a very useful property for testing the validity of any residual caluclation.
    </para>
    
    <para>Calulating residuals are essentual for determining any weightning model that should be applied in any VCV weighting model.
    As it has been seen caculating an optimal VCV weighting model relies heavily on the determination of the standard deviation
    of system according to a Gaussian distribution. One method doing this is to caculate the new weighting matrix as
    
    EQUATION 16
    
    where <command>N</command> is given by
    
    EQUATION 17
    
    This method, however, leads to the new weighting matrix (PUT IN SIGMA HERE!!!) that is not strictly diagonal. This is
    non-optimal as it computationally much slower to calculate. For example, if the follwing matrix mutiplication was to be computed
    
    EQUATION 18
    
    where <command>A</command> was a <command>observations</command> x <command>2</command> matrix and <command>P<command> was a 
    <command>observations</command> x <command>observations</command> weighting matrix. If our weighting matrix was strictly
    diagonal then then it would take
    
    EQUATION 19
    
    operations to compute Equation 1.18. Converserly, if <command>P<command> was a non-diagonal matrix then it would take 
    
    EQUATION 20
    
    operations to compute the same equation (MAYBE GIVE SOME FIGURES IN SECONDS).
    
    </para>
    
    <para>Considering the slowness of using a weighting matrix that is not diagonal, another method of calculating a weighting
    matrix was sought. This method works entirely in the residual space and ensures that the weighting matrix is always diagonal.
    In this caculation a line of best fit is cauclated using Equation 1.7 and the resiudals are calcuated using Equation 1.14. The
    residual matrix is then sorted in ascending order. From there the median and interquartile range can be calculated. The
    threshold for two standard deviations above or below the mean, assuming a standard Gaussian distribution can be calulated as
    
    EQUATION 21
    
    resulting in a box and wiskers plot (see Figure 1.2). Therefore, any resiudal with a value greater than two standard deviations
    above the mean is considered to be errorous and will be weighted out.
    </para>
    
     
  </sect3><!--Residuals and Detetermining VCV Weighting Models-->
</sect2> <!--Theory-->

<sect2><title>Conclusion</title>
  <para>Time domain analyisis of discrete geodetic signals is a powerful tool in determining data trends. Linear Regression Modeling
  can be employed to establish any general patterns in the data and give an indication of any data anomolies. It has also been seen 
  that by using a weighting matrix, erronious can be removed, thereby allowing a more acurate regression model to be fitted. 
  Weighting matricies can also be employed to emphasise any underlying data trends, such as Random Walk to be
  seen. Time Domain analyisis also allows a given system to be modelled in the residual space. This is important for may reasons,
  firstly, the residuals give an indication of how well the regression model actually fits the observation data in both the time and
  frequency domains. In addition, the residual space allows an optimal VCV weighting matrix to be cauclated, such that it is always
  garanteed to be strictly diagonal. This vastly reduces the computaitonal time reqired to caculate a Least Squares Solution.     
  </para>
</sect2> <!--Conclusion-->

</sect1> <!--TDA-->
  <sect1><title>Interpolation</title>

<sect2><title>Introduction</title>
  <para></para>
</sect2> <!--Introduction-->

<sect2><title>Theory</title>
  <para>
    ideas:
    -req'd for data to be FFT'd - FFT data must be regular
    -provides models for periods where data is not available
    -need several techniques as no single technq. suits all applications.
   </para>

</sect2> <!--Theory-->

<sect2><title>Conclusion</title>
  <para></para>
</sect2> <!--Conclusion-->


</sect1> <!--Interpolation-->

  <sect1><title>Windowing</title>

<sect2><title>Introduction</title>
  <para></para>
</sect2> <!--Introduction-->

<sect2><title>Theory</title>
  <para>
   Ideas:
   - Helps reduce spectral leakage for when a signla is transformed into the freq. domain
   - Trade off - between amplitude accuracy, frequency accuracy, and noise reduction.
   - Need several as no single windowing algorithm suits all appplications
  </para>

</sect2> <!--Theory-->

<sect2><title>Conclusion</title>
  <para></para>
</sect2> <!--Conclusion-->

</sect1> <!--Windowing-->

  <sect1><title>Frequency Domain Analysis</title>

<sect2><title>Introduction</title>

<para>
  Fourier transforms enable us to view the frequency representation of data that exists in the time domain. In visual terms, the
  result is frequency on the horizontal axis with amplitude on the vertical. From such a graph, we can see details of what is happening in
  the data at particular frequencies. This provides for opportunities such as the removal of erroneous frequencies in the given data set
  and noise reduction. Once the desired actions have been carried out in the frequency domain, an inverse transform can be applied to
  return the data to the time domain.

  Another use of Fourier Transforms are to produce Power Spectral Density (PSD) plots.

</para>

</sect2> <!--Introduction-->



<sect2><title>Theory</title>

<para>
  The concepts of frequency domain analysis should commence with the Fourier transform. Any waveform can be constructed purely from sine
  and cosine waves. An example of this is a pure square wave. It is constructed from the following:
  <equation>
    <title>Square Wave</title>
    <alt>x(n) = sin(f) + 1/3*sin(3f) + 1/5*(sin5f)</alt>
    <graphic format="eps" fileref="gen-forward_fourier.eps">
  </equation>
</para>

<para>
	...for all odd harmonics. Where f is the fundamental frequency.
</para>

<para>
  It therefore stand to reason that any waveform can be broken down into to these sine and cosine components. The
  Fourier transform is such a tool to carry out this analysis. The Fourier transform effectively analysis a given input signal, by
  decomposing it into those sinusoids of different frequencies, that sum the original signal.
</para>

<para>
  The resulting function from a Fourier transform is function of frequency. This can be expressed in terms of frequency f, or angular
  frequency. The Fourier transform of a signal x(n) is:

  <equation>
    <title>Fourier Transform</title>
    <graphic format="eps" fileref="gen-forward_fourier.eps">
  </equation>
</para>

<para>
  We can also computer the inverse Fourier transform, returning us a function of time:

  <equation>
    <title>Inverse Fourier Transform</title>
    <alt>x(n) = 1/2pi * int( X( e^(jw) ) * e^(jwn) )dw - where w = 2pif</alt>
    <graphic format="eps" fileref="gen-inverse_fourier.eps">
  </equation>
</para>

<para>
  The above transforms, are not however, available for all sequences x(n). This is where the Discrete Fourier transform (DFT) comes in. It
  restricts the sequence x(n) to the following conditions:
</para>

<para>
<itemizedlist>
<listitem><para> The N values x(0) ... x(N-1).</para></listitem>
<listitem><para> Equidistant values around the unit circle.</para></listitem>
</itemizedlist>
</para>

<para>
 The formula for the DFT is:

  <equation>
    <title>Forward Discrete Fourier Transform</title>
    <alt>X(k) = X(e^(j2pik/N)) = sum from n=0 ..N-1 ( x(n)*e^(-j2pikn/N) )</alt>
    <graphic format="eps" fileref="gen-forward_discrete_fourier.eps">
  </equation>

  and the inverse DFT:

  <equation>
    <title>Inverse Discrete Fourier Transform</title>
    <alt>x(n) = 1/N * sum from n-0..N-1(X(k)*e^(j2pikn/N))</alt>
    <graphic format="eps" fileref="gen-inverse_discrete_fourier.eps">
  </equation>

  The discrete frequency k above is given by f(k) = k*fs/N where fs is the sampling frequency of the signal.
</para>

<para>
  The problem with the DFT is that it is slow, with O(N^2). This brought about the Fast Fourier transform.
  The Fast Fourier transform takes into account that the DFT wastes a good deal of time with unnecessary multiplications. For example,
  there is no need to multiply something by zero, when the zero can be used outright. Given that multiplication is one of the slower
  instructions on computers, this is a time saver. Another area where the FFT screams efficiency, is that it takes into account the fact
  that value repeat in sinusoids. For example, every 90 degrees, the value 1 reappears on a sine or cosine changing only its sine. Therefore
  many results may determined by taking into account these factors. Through this increased efficiency, the FFT reduces the computation
  time to O(NlogN).
</para>

<para>
  The power spectrum is very useful in signal processing. It is the most common frequency measuremnt, allowing you to find how much energy
  exists at a given frequency. Derived from the power spectrum is the power spectral density (PSD) measurment. A PSD plot provides
  information on how much energy exists in a band of frequencies. This technique is particularly useful for measuring the noise content
  in a signal. A PSD plot is created by plotting the magnitude of the real and imaginary components of the FFT results.
</para>


</sect2> <!--Theory-->


<sect2><title>Conclusion</title>
  <para></para>
</sect2> <!--Conclusion-->

</sect1> <!--FDA-->

  <sect1><title>User Interface</title>

<sect2><title>Introduction</title>
  <para></para>
</sect2> <!--Introduction-->

<sect2><title>Theory</title>
  <para></para>
</sect2> <!--Theory-->

<sect2><title>Conclusion</title>
  <para></para>
</sect2> <!--Conclusion-->

</sect1> <!--UI-->

</chapter>

<chapter><title>Implementation</title>

  <sect1><title>Design Philosophy</title>

<sect2><title>Introduction</title>

<para>
The first design requirement for the GDMS project, was that it be licensed un the GNU General Public
License. The reason for this is to allow access to the software by people who are not necessarily in a
position to use propriatry software. The GDMS package is complex and relies on a number of external
libraries for mathematical and graphical functionality. Similar propriatry libraries can be very
expensive.
</para>

<para>
The second issue here, which is in fact related to the first, is the need to have a package which is
as platform independant as possible. This contributed significantly to the choice of programming
language. In using  ANSI ISO compliant C++, the GDMS package is intended to run on most Unix based
operating systems. It should also be noted that most modern windowing toolkits require C++.
</para>

<para>
The choice of user interface toolkit also maximized the cross platform capabilities of the application. This is because <command>wxWindows</command> has been specifically built to be easily ported, and currently runs on the following platforms:
</para>

<itemizedlist>
<listitem><para>Microsoft Windows</para></listitem>
<listitem><para>Unix (including Linux)/GTK+</para></listitem>
<listitem><para>Unix (including Linux)/Motif and X11</para></listitem>
<listitem><para>MacOS</para></listitem>
<listitem><para>OS/2</para></listitem>
</itemizedlist>

<para>
The next important design desicion involves the methodologies used to design implement the GDMS
package. Given the complexity of the system, the Object Oriented approach was deemed the most
suitable. Using OO, the package lends itself well to future enhancements, maintenance and
extensibility. In fact, throught the implementation process, changes in design were required to
suit the reuirements of the growing system. These changes, although not always trivial, were faciliated by
the use of OO methodology.
</para>


</sect2> <!--Introduction-->

<sect2><title>Theory</title>

  <para>
    - ANSI ISO stuff
  </para>

  <para>
    - Serious OO talk
   </para>

</sect2> <!--Theory-->

<sect2><title>Future Enahncements</title>
  <!--may want to integrate this with conclusion?-->
  <para></para>
</sect2> <!--Future Enahncements-->

<sect2><title>Conclusion</title>
  <para></para>
</sect2> <!--Conclusion-->

</sect1> <!--design-->

  <sect1><title>Prototype</title>

  <sect2><title>Introduction</title>
    <para>
      Matlab, Functionality etc..
    </para>

    <para>
      The MATLAB prototype was the minimum requirement of the project. It was to carry out all of the essential
      mathematical functionality providing not only a working model, however also acting as a test bed for final
      product functionality. Most of the features implemented in C++ were tested in this way to ensure correct and
      consistent results. Limited time was spent on dressing the prototype as it was completed early enough in the
      project for group to be able turn to the full implementation in C++.
    </para>

  </sect2> <!--Introduction-->

  <sect2><title>Theory</title>
    <para>
      Research also - TSView etc..
    </para>
  </sect2> <!--Theory-->

  <sect2><title>Conclusion</title>
    <para></para>
  </sect2> <!--Conclusion-->

</sect1> <!--Proto-->


  <sect1><title>Implementation Issues</title>
    <!--these sections will sit under sect1 - Implementation Issues-->

<sect2><title>Time Domain Analysis</title>

  <sect3><title>Introduction</title>
    <para></para>
  </sect3> <!--Introduction-->

  <sect3><title>Least Squares and Residuals</title>
    <para></para>
    <sect4><title>Research</title>
      <para>
         - Which algorithms?
         - Asses alternative algorithms
      </para>
    </sect4> <!--research-->
  </sect3> <!--LS/residuals-->

  <sect3><title>Implementation</title>
    <para>
      - Speed/ Optimisations
    </para>
  </sect3> <!--Implementation-->

  <sect3><title>Future Enahncements</title>
    <para>
    </para>
  </sect3> <!--Future Enahncements-->

</sect2> <!--TDA-->


    <!--these sections will sit under sect1 - Implementation Issues-->

<sect2><title>Interpolation</title>

  <para></para>

  <sect3><title>Introduction</title>
    <para></para>
  </sect3> <!--Introduction-->

  <sect3><title>Research</title>
    <para>
     - Splines etc..
     - Which algorithms?
     - Assess alternative algorithms
    </para>
  </sect3> <!--research-->

  <sect3><title>Implementation</title>
    <para>
       - How was it done?
       - Speed/ Optimisations
    </para>
  </sect3> <!--Implementation-->

  <sect3><title>Future Enahncements</title>
    <para>
    </para>
  </sect3> <!--Future Enahncements-->

</sect2> <!--Interp-->

    <!--these sections will sit under sect1 - Implementation Issues-->

<sect2><title>Windowing</title>

  <sect3><title>Introduction</title>
  </sect3> <!--Introduction-->

  <sect3><title>Research</title>
    <para>
      - Rectangular, Blackman, Hamming, Dolph-Chebychev, etc..
      - Which algorithms?
        - Rectangular: inluded for preliminary testing)
        - Blackman: used to compare and contrast with hamming
        - Hamming: a useful and very good algorithm. this provides nice side lobe attenuation
        - Dolph-Chebyshev: another very good algorithm. This possesses a frequency response similar
          to a Hamming window, however provides better attenuation of high frequencies. the minumum
          attenuation of frequencies greater than the sampling frequency for dolph-chebyshev is 63 dB
          as compared to 50 dB for hamming

      - Assess alternative algorithms
        This has been designed to be easily extensible. A new algorithm can be added by specialising
        the cepWindowAlg class and overriding the getValue method.
        A new windowing algorithm can be added at the expense of less than 10 lines of code (with the
        exception of those such as dolph-chebyshev who require alot more attention)
    </para>
  </sect3> <!--research-->

  <sect3><title>Implementation</title>
  <para>
       - How was it done?

                           cepWindowAlg     <-------------------    cepDataWindower
                                 ^
                                 |
              --------------------------------------
              |            |           |           |
          rectangular   blackman    hamming    chebyshev

        - These coefficients can be generated for an arbitrary window size. The requirement that
          the windowed data can be converted to the Fourier domain imposes afuther limitation in that
          the fourier libraries we are using will only operate on a data set of size 2^n ( 0 &amp;< n < inf);
          This restriction has been implmented at the UI level, maintaining the flexibility of the
          windowing subsystem.


        - Blackman - this was added at a simlar time
            coeff(i) = 0.42 - 0.5*cos( 2*PI*val ) + 0.08*cos(4*PI* i/(size-1));
        - Hamming. implemented using the time honoured formula
            coeff(i) = 0.54 - 0.46*cos( 2*PI*i/(size-1) );
        - Dolph-Chebychev

            http://www.maths.tcd.ie/~plynch/Publications/Dolph.pdf
            http://www.isip.msstate.edu/projects/speech/software/documentation/class/algo/Window/
            http://www.octave.org/octave-lists/archive/octave-sources.2002/msg00019.html
            dolph
        - etc..
       - Speed/ Optimisations
          - calulation of window coefficients are performed only when the algorithm is changed, or paramters
            are altered.
          - for the simpler algoritms, speed is not an issue. Th emost expensive of these algorithms [blackman]
            2 cosines and 6 muliplications. this is inexpensive when considering the frequency at which recalulation
            will be required, and the relatively small size of the window required.
          - Dolph-Chebyshev will be more expensive. The code that is implmented is based on a FORTRAN algorithm. This
            has been implmented as faithfully as possible so as to maintain the integrity of the window coefficients.
            The support the ability of the user to request a window of arbitrary size, this particular window algorithm
            implements is own inverse fourier transform.
            <!-- TODO - insert an example of the ifft here -->

  </para>
  </sect3> <!--Implementation-->

  <sect3><title>Future Enahncements</title>
  <para>
  - possibility of adding more windowing algorthms
  - possible optimisation of the dolph-chebyshev code
  </para>
  </sect3> <!--Future Enahncements-->


</sect2> <!--Windowing-->

    <!--these sections will sit under sect1 - Implementation Issues-->

<sect2><title>Frequency Domain Analysis</title>

  <sect3><title>Introduction</title>
    <para></para>
  </sect3> <!--Introduction-->

  <sect3><title>Research</title>
    <para>
       - FFTW etc..
       - Which algorithms?
       - Assess alternative algorithms
    </para>

  <para>
     Why FFT template class?

     A considerable amount of time was spend researching the FFTW (Fastest Fourier
     Transform in the West) library written in C. The library is considered by many sources to be the most
     correct, efficient, and probably the most widely used. Tests were carried out with the library  to
     ensure it could be well integrated with GDMS.
  </para>

  <para>
     During this testing process, another FFT library was found. This was a simple template class written
     in C++. Testing was carried out with this class and it was decided that it would be more easily
     integrated with GDMS, suiting the system's paradigm. During closer inspection and testing of the
     template class, it was discovered that that is did have limitations and required some modifications.
     The necessary changes were made and it was integrated. Although it is quite likely that FFTW has
     greater levels of efficiency and flexibility, the template class offered a more consistent solution
     for GDMS.
   </para>

   <para>
     The group did intend to employ the use of FFTW is time allowed in order to surpass the restrictions of
     the template class, however this was not to be the case.
   </para>

  </sect3> <!--research-->

  <sect3><title>Implementation</title>
     <para>
       - FFT, PSD
       - How was it done?
       - Speed/ Optimisations
    </para>
  </sect3> <!--Implementation-->

  <sect3><title>Future Enahncements</title>
    <para>
    </para>
  </sect3> <!--Future Enahncements-->

</sect2> <!--FDA-->

    <!--these sections will sit under sect1 - Implementation Issues-->

<sect2><title>User Interface</title>

  <sect3><title>Introduction</title>
<para>
The <command>GDMS</command> user interface is the most prominent part of the application. It is the main way in which the user experiences the application. <command>GDMS</command> currently supports two user interface variants:
</para>

<itemizedlist>
<listitem><para>The X windows user interface (the program called <command>gdms</command>)</para></listitem>
<listitem><para>The batch user interface (the program called <command>gdms-batch</command>)</para></listitem>
</itemizedlist>

<para>
This section will mainly discuss the research undertaken for implementing the X windows user interface. This is partially because the scale of implementation required was much greater for this variant, but is also because there were many more alternatives available for this variant.
</para>
  </sect3> <!--Introduction-->

  <sect3><title>Research</title>

<sect4><title>X windows user interface</title>
<para>
wx, qt, gtk, kde, gnome
</para>
</sect4>

<sect4><title>Batch user interface</title>
<para>
There are two common implementation methodologies for scripting languages. Both of these were considered for the <command>GDMS</command> implementation. The most common manner for implementing the parser and grammar for a scripting language is by using the compiler construction tools <command>yacc</command>, and <command>lex</command> (or their free versions, <command>bison</command> and <command>flex</command>). 
</para>

<para>
<command>lex</command> and <command>flex</command> are parser generation tools. They take a lexical specification file, and generate a left associative (TODO CHECK) finite state machine for matching input strings. A sample of a lexical specification is shown below:
</para>

<programlisting>
<INITIAL>\%PDF-[0-9]+\.[0-9]+    { debuglex(yytext, -1, "version");
                                   yylval.sval.data = (char *) returnStr(yytext, -1);
                                   yylval.sval.len = yyleng;
                                   return VERSION;
                                 }

<INITIAL>[+-]?[0-9]+\.[0-9]+     { debuglex(yytext, -1, "floating point");
                                   yylval.sval.data = (char *) returnStr(yytext, yyleng);
                                   yylval.sval.len = yyleng; 
                                   return FP; 
                                 }

<INITIAL>[+-]?[0-9]+\.[0-9]+\>\> { debuglex(yytext, -1, "floating point");
                                   yyless(yyleng - 2);
                                   yylval.sval.data = (char *) returnStr(yytext, yyleng);
                                   yylval.sval.len = yyleng;
                                   return FP;
                                 } 
</programlisting>

<para>
In this example, two tokens are defined, VERSION, and FP. The syntax for these specifications is fairly trivial. The first field, which is wrapped in angled brackets, is the name of the state that the specification should be used in. All of the examples shown here are for the default state, known as INITIAL. What follows this is a posix compliant regular expression to be matched. Finally, within the braces is some c code to execute which a match occurs.
</para>

<para>
In each of these examples, the c code writes a log entry for the match, setups up the data structure symbolizing the match, and then returns the name of the lexicial token which was matched.
</para>

<para>
<command>yacc</command> and <command>bison</command> define the grammer which uses the tokens defined by <command>lex</command> or <command>flex</command>. In effect, this defines valid orders for the tokens to appear in, and what operations should be performed when a set of tokens is matched. Error handling for undefined token combinations is also supplied by the grammar (with hooks for reporting to the user in a manner which is appropriate to that particular application.
</para>

<para>
A sample grammar specification is as follows:
</para>

<programlisting>

</programlisting>
</sect4>

  </sect3> <!--research-->

  <sect3><title>Implementation</title>
  <para>
       - How was it done?
       - Speed/ Optimisations
  </para>      
  </sect3> <!--Implementation-->
  
  <sect3><title>Integration</title>
  </sect3> <!--Integration-->


  <sect3><title>Future Enahncements</title>
  <para>
  </para>
  </sect3> <!--Future Enahncements-->


</sect2> <!--UI-->

  </sect1>

</chapter>

<chapter><title>Testing</title>
  <!--these sections will sit under sect1 - Implementation Issues-->

<sect1><title>Testing</title>

  <sect2><title>User Interface Testing</title>
  
    <sect3><title>Introduction</title>
      <para></para>
    </sect3> <!--Introduction-->

    <sect3><title>Future Enahncements</title>
    <!--Do we want/need this here?-->
      <para></para>
    </sect3> <!--Future Enahncements-->

  </sect2> <!--UI Testing-->


  <sect2><title>Unit Testing</title>

    <sect3><title>Introduction</title>
      <para></para>
    </sect3> <!--Introduction-->

    <sect3><title>Future Enahncements</title>
    <!--Do we want/need this here?-->
    <para></para>
    </sect3> <!--Future Enahncements-->
  </sect2> <!--UI Testing-->

</sect1> <!--Testing-->

</chapter>

<chapter><title>Conclusion</title>
  <sect1><title>Conclusion</title>
<para>
<itemizedlist>
  <listitem><para>What did we do?</para></listitem>
  <listitem><para>Did we succeed?</para></listitem>
  <listitem><para>Good or bad result?</para></listitem>
  <listitem><para>What did we learn?</para></listitem>
  <listitem><para>Summarise lessons</para></listitem>
  <listitem><para>Summarise future enhancements</para></listitem>
</itemizedlist>
</para>
</sect1> <!--Conclusion-->

</chapter>

</book>
